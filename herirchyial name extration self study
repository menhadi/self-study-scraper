import csv
import os
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --------------------------------------------
# CONFIGURATION
# --------------------------------------------
INPUT_FILE = "urls (2).csv"     # Input file with URLs
URL_COLUMN = "URL"              # Column name for URLs
LEVEL1_CLASS = "sample-links mb-3 ul1"
LEVEL2_CLASS = "colored-links mb-3"
OUTPUT_CSV = "hierarchy_output_fast.csv"
OUTPUT_TXT = "hierarchy_output_fast.txt"
SAVE_INTERVAL = 25              # Save every 25 URLs (less I/O = faster)
# --------------------------------------------


# =====================================================
# SETUP & UTILITY
# =====================================================

def start_selenium():
    """Start a single persistent Selenium instance."""
    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(25)
    return driver


def stop_selenium(driver):
    """Safely stop Selenium."""
    try:
        driver.quit()
    except:
        pass


# Persistent HTTP session (faster than re-opening each time)
session = requests.Session()
session.headers.update({"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"})


# =====================================================
# FETCHING
# =====================================================

def get_html(url, driver=None):
    """Fetch HTML: Try requests first, fallback to Selenium for JS-heavy accordions."""
    try:
        res = session.get(url, timeout=8)
        if res.status_code == 200 and "<ul" in res.text:
            return res.text
    except Exception:
        pass

    if not driver:
        return None

    print(f"‚öôÔ∏è Using Selenium for {url}")
    try:
        driver.get(url)
        # Smart wait instead of fixed sleep
        WebDriverWait(driver, 6).until(
            EC.presence_of_element_located((By.TAG_NAME, "ul"))
        )
        return driver.page_source
    except Exception as e:
        print(f"‚ö†Ô∏è Selenium load failed: {e}")
        return None


# =====================================================
# PARSING (4-Level Accordion)
# =====================================================

def extract_level1_links(html, base_url):
    """
    Recursively extract up to 4 hierarchical levels:
      Parent, Child, SubChild, SubSubChild
    Returns list of tuples: (L1, L2, L3, L4, link_for_this_node)
    Emits an entry for a node even if it has children (so you can see the node's own link)
    """
    soup = BeautifulSoup(html, "html.parser")
    top_uls = soup.find_all("ul", class_=LEVEL1_CLASS)
    data = []

    def is_nested_ul_of_interest(u):
        # Accept any UL that looks like an accordion or nested list.
        cls = u.get("class", [])
        # if no class, still OK (some pages may not include class)
        return True

    def recurse_list(ul, parents):
        for li in ul.find_all("li", recursive=False):
            a = li.find("a", recursive=False)
            if not a:
                # If there's no anchor, try to find text directly
                text = li.get_text(strip=True)
                href = ""
            else:
                text = a.get_text(strip=True)
                href = a.get("href", "").strip()
            full_link = urljoin(base_url, href)

            # Find nested ULs (direct children only)
            nested_ul = None
            for possible_ul in li.find_all("ul", recursive=False):
                if is_nested_ul_of_interest(possible_ul):
                    nested_ul = possible_ul
                    break

            # Always emit current node as an entry (so parent nodes with children still appear)
            levels = parents + [text]
            # Pad to 4 levels
            while len(levels) < 4:
                levels.append("")
            data.append((levels[0], levels[1], levels[2], levels[3], full_link))

            # If nested UL exists, recurse to produce deeper entries
            if nested_ul:
                recurse_list(nested_ul, parents + [text])

    for ul in top_uls:
        recurse_list(ul, [])

    return data


def extract_level2_links(html, base_url):
    """Extract SubChild/Chapter links (handles multiple layouts)."""
    if not html:
        return []
    soup = BeautifulSoup(html, "html.parser")
    results = []
    for cls in [LEVEL2_CLASS, "box-links mb-3"]:
        uls = soup.find_all("ul", class_=cls)
        for ul in uls:
            for li in ul.find_all("li", recursive=False):
                a = li.find("a")
                if not a:
                    continue
                text = a.get_text(strip=True)
                href = a.get("href", "").strip()
                full_link = urljoin(base_url, href)
                results.append((text, full_link))
    return results


# =====================================================
# FILE MANAGEMENT
# =====================================================

def load_input_urls():
    """Load URLs from input CSV."""
    df = pd.read_csv(INPUT_FILE, encoding="utf-8-sig")
    if URL_COLUMN not in df.columns:
        raise ValueError(f"‚ùå Missing column '{URL_COLUMN}' in {INPUT_FILE}")
    return df


def append_to_files(rows, csv_headers):
    """Append extracted rows to CSV and TXT (batch mode)."""
    csv_exists = os.path.exists(OUTPUT_CSV)
    with open(OUTPUT_CSV, "a", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        if not csv_exists:
            writer.writerow(csv_headers)
        writer.writerows(rows)

    with open(OUTPUT_TXT, "a", encoding="utf-8-sig") as f:
        for row in rows:
            breadcrumb = row[-1]
            # last 4 URL columns are at positions: -5 .. -2 depending on header layout
            # We will pick the deepest non-empty URL among SubSubChild, SubChild, Child, Parent
            parent_url = row[-7] if len(row) >= 7 else ""
            child_url = row[-6] if len(row) >= 6 else ""
            subchild_url = row[-5] if len(row) >= 5 else ""
            subsub_url = row[-4] if len(row) >= 4 else ""
            chosen = subsub_url or subchild_url or child_url or parent_url or ""
            f.write(f"{breadcrumb} -> {chosen}\n")


# =====================================================
# MAIN CRAWLER (FAST MODE)
# =====================================================

def crawl_all():
    """Main crawler logic (optimized version)."""
    df = load_input_urls()
    results_batch = []
    processed = 0

    driver = start_selenium()

    for idx, row in df.iterrows():
        main_url = str(row[URL_COLUMN]).strip()
        if not main_url or not main_url.startswith("http"):
            continue

        meta_cols = [str(x) if str(x) != "nan" else "" for x in row]

        print(f"\nüîç Processing ({idx+1}/{len(df)}): {main_url}")

        try:
            main_html = get_html(main_url, driver)
            if not main_html:
                print(f"‚ö†Ô∏è Empty page at {main_url}")
                continue

            level1_entries = extract_level1_links(main_html, main_url)

            if not level1_entries:
                results_batch.append([
                    *meta_cols, "", "", "", "",
                    main_url, "", "", "", "",
                    f"{main_url}"
                ])

            for L1, L2, L3, L4, node_link in level1_entries:
                # Breadcrumb only includes non-empty levels
                breadcrumb = " > ".join([x for x in [L1, L2, L3, L4] if x])

                # Prepare output row:
                # Meta cols + L1,L2,L3,L4 + Parent URL, Child URL, SubChild URL, SubSubChild URL + Breadcrumb
                # We follow the convention:
                #  - Parent URL = main_url
                #  - Child/ deeper URL = node_link (the page where that node points)
                #    (for nodes that are deeper, node_link belongs to that deeper column)
                parent_url = main_url
                child_url = ""
                subchild_url = ""
                subsub_url = ""

                # Map node_link to the deepest non-empty level in L1..L4
                if L4:
                    subsub_url = node_link if node_link and "javascript:void(0)" not in node_link else ""
                elif L3:
                    subchild_url = node_link if node_link and "javascript:void(0)" not in node_link else ""
                elif L2:
                    child_url = node_link if node_link and "javascript:void(0)" not in node_link else ""
                else:
                    # top-level link
                    child_url = node_link if node_link and "javascript:void(0)" not in node_link else ""

                results_batch.append([
                    *meta_cols,
                    L1, L2, L3, L4,
                    parent_url, child_url, subchild_url, subsub_url,
                    breadcrumb
                ])

                # If node_link is visitable, fetch level-2 style links (chapters) from that page
                visit_link = node_link
                if not visit_link or "javascript:void(0)" in visit_link:
                    continue

                sub_html = get_html(visit_link, driver)
                level2_links = extract_level2_links(sub_html, visit_link)

                # Determine current deepest level index (0-based): 0..3
                current_levels = [L1, L2, L3, L4]
                deepest_idx = 0
                for i in range(4):
                    if current_levels[i]:
                        deepest_idx = i

                for sub_text, sub_link in level2_links:
                    # Create a new levels copy and put this sub_text into next depth (cap at index 3)
                    new_levels = current_levels.copy()
                    next_idx = deepest_idx + 1
                    if next_idx > 3:
                        next_idx = 3
                    new_levels[next_idx] = sub_text

                    # Map URLs for the new row:
                    # Parent URL remains main_url
                    # Child URL = visit_link (the page we visited)
                    # SubChild URL = sub_link (the chapter link)
                    new_parent_url = main_url
                    new_child_url = visit_link if deepest_idx >= 0 else ""
                    new_subchild_url = sub_link if next_idx >= 2 else ""
                    new_subsub_url = sub_link if next_idx == 3 else ""

                    breadcrumb2 = " > ".join([x for x in new_levels if x])

                    results_batch.append([
                        *meta_cols,
                        new_levels[0], new_levels[1], new_levels[2], new_levels[3],
                        new_parent_url, new_child_url, new_subchild_url, new_subsub_url,
                        breadcrumb2
                    ])

                processed += 1
                if len(results_batch) >= SAVE_INTERVAL:
                    append_to_files(
                        results_batch,
                        list(df.columns) + [
                            "Parent", "Child", "SubChild", "SubSubChild",
                            "Parent URL", "Child URL", "SubChild URL", "SubSubChild URL",
                            "Breadcrumb"
                        ]
                    )
                    print(f"üíæ Progress saved ({processed} URLs).")
                    results_batch = []

        except Exception as e:
            print(f"‚ö†Ô∏è Error on {main_url}: {e}")
            continue

    if results_batch:
        append_to_files(
            results_batch,
            list(df.columns) + [
                "Parent", "Child", "SubChild", "SubSubChild",
                "Parent URL", "Child URL", "SubChild URL", "SubSubChild URL",
                "Breadcrumb"
            ]
        )

    stop_selenium(driver)
    print(f"\n‚úÖ Done! Crawled {len(df)} URLs.")
    print(f"üìÅ Output saved to:\n   - {OUTPUT_CSV}\n   - {OUTPUT_TXT}")


# =====================================================
# RUN
# =====================================================
if __name__ == "__main__":
    crawl_all()
