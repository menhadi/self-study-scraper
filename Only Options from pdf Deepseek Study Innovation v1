import requests
import json
import re
import os
import time
import csv
import PyPDF2
import pdfplumber
from typing import Dict, List, Tuple, Optional
from google.oauth2 import service_account
from googleapiclient.discovery import build
import io

# ==============================
#  CONFIGURATION
# ==============================
DEEPSEEK_API_KEY = "sk-467f5288c9ef40a4ae6ccec5978019ea"  # REPLACE WITH YOUR KEY
EXTRACTION_MODEL = "deepseek-chat"
BASE_URL = "https://api.deepseek.com/v1/chat/completions"

GOOGLE_SHEET_ID = "1U6gW0yqh3GZlkyxvhF_5k3sXMP8GwZT-TNsXqKv7S1o"
CREDENTIALS_FILE = "service-account.json"
SHEET_TAB = "Sheet4"

PROGRESS_FILE = "processed_tables.csv"
UPDATE_BATCH_SIZE = 3

# ==============================
#  PDF TEXT EXTRACTOR
# ==============================
class PDFTextExtractor:
    def __init__(self):
        pass
    
    def extract_text_with_tables(self, pdf_bytes: bytes) -> Tuple[str, List[Dict]]:
        """Extract text with table detection"""
        text = ""
        page_info = []
        
        try:
            with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                for page_num, page in enumerate(pdf.pages, 1):
                    try:
                        # Extract text
                        page_text = page.extract_text() or ""
                        
                        # Try to extract tables
                        tables = page.extract_tables()
                        table_texts = []
                        
                        for table in tables:
                            if table:
                                # Convert table to readable text
                                for row in table:
                                    if any(cell for cell in row):
                                        row_text = " | ".join(str(cell) for cell in row if cell)
                                        table_texts.append(row_text)
                        
                        # Combine text and tables
                        if table_texts:
                            page_text += "\n" + "\n".join(table_texts)
                        
                        if page_text.strip():
                            page_text = self._clean_text(page_text)
                            text += f"\n--- Page {page_num} ---\n{page_text}\n"
                            page_info.append({
                                'page': page_num,
                                'text': page_text,
                                'has_tables': len(table_texts) > 0,
                                'char_count': len(page_text),
                                'words': len(page_text.split())
                            })
                        
                    except Exception as e:
                        error_msg = str(e)[:100]
                        print(f"Warning: Error on page {page_num}: {error_msg}")
                        continue
            
            # Fallback to PyPDF2
            if not text.strip():
                try:
                    reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
                    for page_num, page in enumerate(reader.pages, 1):
                        page_text = page.extract_text()
                        if page_text and page_text.strip():
                            page_text = self._clean_text(page_text)
                            text += f"\n--- Page {page_num} ---\n{page_text}\n"
                            page_info.append({
                                'page': page_num,
                                'text': page_text,
                                'has_tables': self._page_has_tables(page_text),
                                'char_count': len(page_text),
                                'words': len(page_text.split())
                            })
                except:
                    pass
            
            return text, page_info
            
        except Exception as e:
            print(f"Error: PDF extraction error: {str(e)}")
            return "", []
    
    def _clean_text(self, text: str) -> str:
        """Clean extracted text"""
        if not text:
            return ""
        
        # Remove null bytes
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        text = re.sub(r'\ufeff', '', text)
        text = re.sub(r'ï¿½', '', text)
        
        # Clean whitespace
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        return text.strip()
    
    def _page_has_tables(self, page_text: str) -> bool:
        """Check if page has table-like content"""
        if not page_text or len(page_text) < 50:
            return False
        
        # Table patterns
        patterns = [
            r'\|\s*[A-Z]+\s*\|\s*Q\.',  # | Section | Q.
            r'Q\s*\|\s*\d',  # Q | 1
            r'A\s*\|\s*[A-D1-4]',  # A | 4
            r'\|\s*\d+\s*\|\s*[A-D1-4]',  # | 1 | A |
            r'Section\s+Q\.',  # Section Q.
            r'Key\s+/\s+Range',  # Key / Range
        ]
        
        for pattern in patterns:
            if re.search(pattern, page_text, re.IGNORECASE):
                return True
        
        return False

# ==============================
#  TABLE ANSWER KEY EXTRACTOR
# ==============================
class TableAnswerKeyExtractor:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        self.pdf_extractor = PDFTextExtractor()
    
    def download_pdf(self, pdf_url: str) -> Optional[bytes]:
        """Download PDF from URL"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"Downloading PDF (attempt {attempt + 1}/{max_retries})...")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'application/pdf',
                    'Accept-Language': 'en-US,en;q=0.5',
                }
                
                response = requests.get(pdf_url, headers=headers, timeout=60)
                response.raise_for_status()
                
                pdf_bytes = response.content
                
                if len(pdf_bytes) < 100:
                    print(f"Warning: PDF is very small ({len(pdf_bytes)} bytes)")
                    return None
                
                print(f"Downloaded {len(pdf_bytes):,} bytes")
                return pdf_bytes
                
            except Exception as e:
                error_msg = str(e)[:200]
                print(f"Download error (attempt {attempt + 1}): {error_msg}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    return None
        
        return None
    
    def extract_from_pdf(self, pdf_bytes: bytes, file_name: str) -> List[Dict]:
        """Extract table data from PDF"""
        print(f"\nExtracting from: {file_name}")
        
        # Extract text
        text, page_info = self.pdf_extractor.extract_text_with_tables(pdf_bytes)
        
        if not text or len(text.strip()) < 100:
            print("Insufficient text extracted")
            return [{"error": "Insufficient text", "file_name": file_name}]
        
        print(f"Extracted {len(text):,} characters from {len(page_info)} pages")
        
        # Save debug file
        safe_filename = file_name.replace('/', '_').replace('\\', '_')
        debug_file = f"debug_{safe_filename}.txt"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(text[:5000])
        print(f"Saved extracted text to: {debug_file}")
        
        # Get pages with tables
        table_pages = [p for p in page_info if p.get('has_tables')]
        if not table_pages:
            print("No table pages detected, processing all pages")
            table_pages = page_info
        
        print(f"Processing {len(table_pages)} pages with tables")
        
        # Process entire text
        all_entries = self._process_table_text(text, file_name)
        
        # Fallback: Try page by page
        if len(all_entries) < 5:
            print("\nTrying page-by-page extraction...")
            page_entries = self._process_page_by_page(table_pages, file_name)
            all_entries.extend(page_entries)
        
        # Remove duplicates
        unique_entries = self._deduplicate_entries(all_entries)
        
        print(f"\nTotal extracted: {len(unique_entries)} table entries")
        
        return unique_entries
    
    def _process_table_text(self, text: str, file_name: str) -> List[Dict]:
        """Process table text"""
        entries = []
        
        # Split into chunks for API
        chunks = self._split_table_text(text)
        
        for chunk_num, chunk in enumerate(chunks, 1):
            print(f"Processing table chunk {chunk_num}/{len(chunks)}")
            
            chunk_entries = self._extract_from_chunk(chunk, file_name, chunk_num)
            if chunk_entries:
                entries.extend(chunk_entries)
            
            time.sleep(1)
        
        return entries
    
    def _split_table_text(self, text: str) -> List[str]:
        """Split text into chunks"""
        chunks = []
        max_chunk_size = 3000
        
        # Split at natural boundaries
        lines = text.split('\n')
        current_chunk = []
        current_size = 0
        
        for line in lines:
            line_size = len(line)
            
            if current_size + line_size > max_chunk_size and current_chunk:
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_size = line_size
            else:
                current_chunk.append(line)
                current_size += line_size
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    def _extract_from_chunk(self, chunk_text: str, file_name: str, chunk_num: int) -> List[Dict]:
        """Extract table entries using DeepSeek API"""
        prompt = f"""EXTRACT TABLE DATA from answer key PDF.

I need you to extract ALL question-answer pairs from tables like:

TYPE 1: Grid format
Q | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11
A | 4 | 4 | 3 | 1 | 1 | 3 | 3 | 1 | 1 | 2 | 1

TYPE 2: Detailed table format
| Section | Q. No. | Key / Range | Marks |
| GA | 1 | A | 1 |
| GA | 2 | B | 1 |
| GA | 3 | D | 1 |

RULES:
1. Extract EVERY question number and its corresponding answer
2. For grid format: Q row has numbers, A row has answers
3. For detailed format: Extract Section, Q.No., Key/Range, Marks
4. If answer is a range (like "1300 to 1300"), extract as is
5. Include section information when available

FORMAT for each entry:
{{
  "question_number": "1",
  "answer": "4",  # or "A", "B", "C", "D", or range like "1300 to 1300"
  "section": "GA",  # if available
  "marks": "1",  # if available
  "file_name": "{file_name}"
}}

If multiple tables on same page, extract all.

TEXT TO PROCESS:
{chunk_text[:2500]}

Return JSON array.
"""
        
        payload = {
            "model": EXTRACTION_MODEL,
            "temperature": 0.1,
            "messages": [
                {
                    "role": "system",
                    "content": "You extract table data from answer keys. Extract all question-answer pairs from tables."
                },
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 3000
        }
        
        try:
            response = requests.post(BASE_URL, headers=self.headers, json=payload, timeout=90)
            
            if response.status_code != 200:
                return []
            
            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            if not content:
                return []
            
            # Parse JSON
            try:
                content = re.sub(r'```json|```', '', content)
                content = re.sub(r'^\s*json\s*', '', content, flags=re.IGNORECASE)
                
                match = re.search(r'\[\s*\{.*?\}\s*\]', content, re.DOTALL)
                if match:
                    json_str = match.group(0)
                    json_str = re.sub(r',\s*}', '}', json_str)
                    json_str = re.sub(r',\s*]', ']', json_str)
                    entries = json.loads(json_str)
                    
                    # Add metadata
                    for entry in entries:
                        entry['file_name'] = file_name
                        entry['chunk'] = chunk_num
                        entry['extraction_method'] = 'api'
                    
                    return entries
            except:
                pass
            
            # Fallback: Try regex parsing
            return self._regex_extraction(chunk_text, file_name, chunk_num)
            
        except Exception as e:
            print(f"API error: {str(e)[:100]}")
            return self._regex_extraction(chunk_text, file_name, chunk_num)
    
    def _regex_extraction(self, text: str, file_name: str, chunk_num: int) -> List[Dict]:
        """Extract using regex patterns"""
        entries = []
        
        # Pattern 1: Grid format Q | 1 | 2 | 3 ... A | 4 | 4 | 3 ...
        grid_pattern = r'Q\s*[|:]\s*([\d\s|]+)\s*\n\s*A\s*[|:]\s*([A-D\d\s|to\-\.]+)'
        grid_matches = re.finditer(grid_pattern, text, re.IGNORECASE)
        
        for match in grid_matches:
            q_row = match.group(1)
            a_row = match.group(2)
            
            # Extract numbers from Q row
            q_numbers = re.findall(r'\d+', q_row)
            
            # Extract answers from A row
            a_values = []
            # Match numbers or letters or ranges
            a_matches = re.finditer(r'([A-D]|\d+(?:\s*to\s*\d+)?)', a_row, re.IGNORECASE)
            for a_match in a_matches:
                a_values.append(a_match.group(1))
            
            # Pair them up
            for i, (q_num, a_val) in enumerate(zip(q_numbers, a_values)):
                entry = {
                    "question_number": q_num,
                    "answer": a_val.upper() if len(a_val) == 1 and a_val.isalpha() else a_val,
                    "section": "",
                    "marks": "",
                    "file_name": file_name,
                    "chunk": chunk_num,
                    "extraction_method": "regex_grid"
                }
                entries.append(entry)
        
        # Pattern 2: Detailed table format
        table_pattern = r'\|\s*([A-Z]+)\s*\|\s*(\d+)\s*\|\s*([^|]+)\s*\|\s*([^|]+)\s*\|'
        table_matches = re.finditer(table_pattern, text, re.IGNORECASE)
        
        for match in table_matches:
            entry = {
                "question_number": match.group(2),
                "answer": match.group(3).strip(),
                "section": match.group(1),
                "marks": match.group(4).strip(),
                "file_name": file_name,
                "chunk": chunk_num,
                "extraction_method": "regex_table"
            }
            entries.append(entry)
        
        # Pattern 3: Simple Q: A format
        simple_pattern = r'Q\s*[:\.]\s*(\d+)[^:]*[:=]\s*([A-D\d]+)'
        simple_matches = re.finditer(simple_pattern, text, re.IGNORECASE)
        
        for match in simple_matches:
            entry = {
                "question_number": match.group(1),
                "answer": match.group(2),
                "section": "",
                "marks": "",
                "file_name": file_name,
                "chunk": chunk_num,
                "extraction_method": "regex_simple"
            }
            entries.append(entry)
        
        return entries
    
    def _process_page_by_page(self, page_info: List[Dict], file_name: str) -> List[Dict]:
        """Process page by page"""
        entries = []
        
        for page in page_info:
            page_text = page.get('text', '')
            page_num = page.get('page', 0)
            
            if not page_text:
                continue
            
            print(f"Processing page {page_num}")
            
            # Extract from this page
            page_entries = self._regex_extraction(page_text, file_name, page_num)
            entries.extend(page_entries)
        
        return entries
    
    def _deduplicate_entries(self, entries: List[Dict]) -> List[Dict]:
        """Remove duplicate entries"""
        unique_dict = {}
        
        for entry in entries:
            if not isinstance(entry, dict):
                continue
            
            q_num = entry.get('question_number', '')
            if not q_num:
                continue
            
            # Clean question number
            q_num = re.sub(r'[^\d]', '', q_num)
            if not q_num:
                continue
            
            # Keep the most complete entry
            if q_num not in unique_dict:
                unique_dict[q_num] = entry
            else:
                existing = unique_dict[q_num]
                # Prefer entry with more fields filled
                existing_fields = sum(1 for v in existing.values() if v)
                new_fields = sum(1 for v in entry.values() if v)
                if new_fields > existing_fields:
                    unique_dict[q_num] = entry
        
        # Sort by question number
        sorted_entries = []
        for q_num in sorted(unique_dict.keys(), key=lambda x: int(x) if x.isdigit() else 9999):
            sorted_entries.append(unique_dict[q_num])
        
        return sorted_entries
    
    def process_batch(self, pdf_batch: List[Tuple[str, str]]) -> List[Dict]:
        """Process a batch of PDFs"""
        results = []
        
        for file_name, pdf_url in pdf_batch:
            print(f"\n{'='*60}")
            print(f"PROCESSING: {file_name}")
            print(f"URL: {pdf_url[:100]}...")
            print(f"{'='*60}")
            
            try:
                # Download PDF
                pdf_bytes = self.download_pdf(pdf_url)
                if not pdf_bytes:
                    results.append({"error": "Failed to download PDF", "file_name": file_name})
                    continue
                
                # Extract table data
                entries = self.extract_from_pdf(pdf_bytes, file_name)
                results.extend(entries)
                
                time.sleep(2)
                
            except Exception as e:
                error_msg = f"Error processing {file_name}: {str(e)[:200]}"
                print(f"{error_msg}")
                results.append({"error": error_msg, "file_name": file_name})
        
        return results

# ==============================
#  GOOGLE SHEETS PROCESSOR
# ==============================
class GoogleSheetsTableProcessor:
    def __init__(self, credentials_file: str, sheet_id: str, api_key: str, sheet_tab: str):
        self.sheet_id = sheet_id
        self.sheet_tab = sheet_tab
        self.extractor = TableAnswerKeyExtractor(api_key)
        self.service = self._authenticate(credentials_file)
        self.next_output_row = 2
    
    def _authenticate(self, credentials_file: str):
        """Authenticate with Google Sheets API"""
        try:
            if not os.path.exists(credentials_file):
                raise FileNotFoundError(f"Credentials file not found: {credentials_file}")
            
            scopes = ['https://www.googleapis.com/auth/spreadsheets']
            creds = service_account.Credentials.from_service_account_file(
                credentials_file, scopes=scopes)
            return build('sheets', 'v4', credentials=creds)
        except Exception as e:
            print(f"Google Sheets authentication failed: {e}")
            raise
    
    def read_sheet_data(self) -> List[List[str]]:
        """Read file names and URLs from sheet"""
        try:
            result = self.service.spreadsheets().values().get(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!A:B"
            ).execute()
            
            values = result.get('values', [])
            print(f"Read {len(values)} rows from sheet")
            return values
        except Exception as e:
            print(f"Error reading sheet: {e}")
            return []
    
    def load_processed_files(self) -> set:
        """Load already processed files"""
        processed_files = set()
        
        if os.path.exists(PROGRESS_FILE):
            try:
                with open(PROGRESS_FILE, 'r', newline='', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    for row in reader:
                        if row and row[0]:
                            processed_files.add(row[0])
                print(f"Loaded {len(processed_files)} processed files")
            except Exception as e:
                print(f"Error reading progress file: {e}")
        
        return processed_files
    
    def save_processed_files(self, processed_files: set):
        """Save processed files to CSV"""
        try:
            with open(PROGRESS_FILE, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                for file_name in sorted(processed_files):
                    writer.writerow([file_name])
            print(f"Saved {len(processed_files)} processed files")
        except Exception as e:
            print(f"Error saving progress file: {e}")
    
    def clear_output_columns(self):
        """Clear output columns"""
        try:
            # Clear columns C through G (for section, question, answer, marks, method)
            self.service.spreadsheets().values().clear(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!C:G"
            ).execute()
            print("Cleared previous output columns (C-G)")
        except Exception as e:
            print(f"Error clearing columns: {e}")
    
    def update_sheet_with_entries(self, entries_batch: List[Dict]):
        """Update Google Sheets with table entries"""
        if not entries_batch:
            return
        
        rows = []
        for entry in entries_batch:
            if not isinstance(entry, dict):
                continue
            
            if "error" in entry:
                rows.append([
                    "ERROR",
                    entry.get("file_name", ""),
                    entry.get("error", "")
                ])
            else:
                rows.append([
                    entry.get("section", ""),
                    entry.get("question_number", ""),
                    entry.get("answer", ""),
                    entry.get("marks", ""),
                    entry.get("extraction_method", "")
                ])
        
        try:
            if rows:
                range_name = f"{self.sheet_tab}!C{self.next_output_row}"
                body = {"values": rows}
                
                self.service.spreadsheets().values().update(
                    spreadsheetId=self.sheet_id,
                    range=range_name,
                    valueInputOption="RAW",
                    body=body
                ).execute()
                
                print(f"Updated sheet with {len(rows)} entries (row {self.next_output_row})")
                self.next_output_row += len(rows)
        except Exception as e:
            print(f"Error updating sheet: {e}")
    
    def process_all_files(self, batch_size: int = 1, resume: bool = True):
        """Main processing function"""
        print(f"\n{'='*80}")
        print("STARTING TABLE ANSWER KEY EXTRACTION")
        print(f"{'='*80}")
        
        # Read sheet data
        sheet_data = self.read_sheet_data()
        if len(sheet_data) < 2:
            print("No data found in sheet")
            return
        
        # Load processed files
        processed_files = self.load_processed_files()
        
        # Clear output if not resuming
        if not resume or not processed_files:
            self.clear_output_columns()
            self.next_output_row = 2
            print("Starting fresh processing")
        else:
            print(f"Resuming from previous progress ({len(processed_files)} files already processed)")
        
        # Prepare files to process
        files_to_process = []
        skipped_count = 0
        
        for i, row in enumerate(sheet_data[1:], start=2):
            if len(row) < 2:
                continue
            
            file_name = row[0].strip()
            pdf_url = row[1].strip()
            
            if not pdf_url.startswith("http"):
                continue
            
            if resume and file_name in processed_files:
                skipped_count += 1
                continue
            
            files_to_process.append((file_name, pdf_url))
        
        print(f"\nPROCESSING SUMMARY:")
        print(f"  Total rows: {len(sheet_data) - 1}")
        print(f"  Already processed: {skipped_count}")
        print(f"  To process: {len(files_to_process)}")
        
        if not files_to_process:
            print("\nAll files already processed!")
            return
        
        # Process in batches
        all_entries = []
        
        for i in range(0, len(files_to_process), batch_size):
            batch = files_to_process[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            print(f"\n{'='*60}")
            print(f"BATCH {batch_num}")
            
            # Process batch
            batch_results = self.extractor.process_batch(batch)
            
            # Mark files as processed
            for name, _ in batch:
                processed_files.add(name)
            
            self.save_processed_files(processed_files)
            
            # Filter valid entries
            valid_entries = [e for e in batch_results if isinstance(e, dict) and e.get("question_number")]
            all_entries.extend(valid_entries)
            
            # Update sheet
            if valid_entries:
                self.update_sheet_with_entries(valid_entries)
            
            # Rate limiting
            if i + batch_size < len(files_to_process):
                time.sleep(3)
        
        # Final summary
        print(f"\n{'='*80}")
        print("PROCESSING COMPLETE!")
        print(f"{'='*80}")
        print(f"Total files processed: {len(files_to_process)}")
        print(f"Total entries extracted: {len(all_entries)}")
        print(f"Entries saved to: {self.sheet_tab}!C{2}:G{self.next_output_row-1}")
        print(f"{'='*80}")

# ==============================
#  MAIN FUNCTION
# ==============================
def main():
    print("\n" + "="*80)
    print("   TABLE ANSWER KEY EXTRACTION SYSTEM")
    print("="*80)
    
    if DEEPSEEK_API_KEY == "your_deepseek_api_key_here":
        print("ERROR: Please update your DeepSeek API key")
        return
    
    if not os.path.exists(CREDENTIALS_FILE):
        print(f"\nCredentials file not found: {CREDENTIALS_FILE}")
        return
    
    # Configuration
    BATCH_SIZE = 1
    RESUME_MODE = True
    
    print(f"\nCONFIGURATION:")
    print(f"  Model: {EXTRACTION_MODEL}")
    print(f"  Sheet: {GOOGLE_SHEET_ID}")
    print(f"  Tab: {SHEET_TAB}")
    
    try:
        processor = GoogleSheetsTableProcessor(
            credentials_file=CREDENTIALS_FILE,
            sheet_id=GOOGLE_SHEET_ID,
            api_key=DEEPSEEK_API_KEY,
            sheet_tab=SHEET_TAB
        )
        
        processor.process_all_files(batch_size=BATCH_SIZE, resume=RESUME_MODE)
        
    except KeyboardInterrupt:
        print("\n\nProcessing interrupted")
    except Exception as e:
        print(f"\nFatal error: {e}")
        import traceback
        traceback.print_exc()

# ==============================
#  TEST FUNCTION
# ==============================
def test_single_file():
    """Test single file extraction"""
    print("\n" + "="*80)
    print("   TEST SINGLE FILE")
    print("="*80)
    
    if DEEPSEEK_API_KEY == "your_deepseek_api_key_here":
        print("Please update API key")
        return
    
    print("\nEnter PDF URL:")
    pdf_url = input().strip()
    
    if not pdf_url:
        print("No URL provided")
        return
    
    extractor = TableAnswerKeyExtractor(DEEPSEEK_API_KEY)
    
    pdf_bytes = extractor.download_pdf(pdf_url)
    if not pdf_bytes:
        print("Failed to download")
        return
    
    entries = extractor.extract_from_pdf(pdf_bytes, "test.pdf")
    
    # Save results
    output_file = "test_table_entries.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(entries, f, indent=2, ensure_ascii=False)
    
    print(f"\nResults saved to: {output_file}")
    print(f"\nExtracted {len([e for e in entries if isinstance(e, dict)])} entries")
    
    # Show samples
    print("\nFirst 10 entries:")
    for i, entry in enumerate(entries[:10]):
        if isinstance(entry, dict):
            print(f"{i+1:2}. Q{entry.get('question_number', '?'):>3}: {entry.get('answer', '?'):<10} "
                  f"Section: {entry.get('section', 'N/A'):<3} Marks: {entry.get('marks', 'N/A'):<3}")

# ==============================
#  ENTRY POINT
# ==============================
if __name__ == "__main__":
    print("Choose mode:")
    print("1. Process table data to Google Sheets")
    print("2. Test single file")
    print("3. Exit")
    
    choice = input("\nEnter choice (1/2/3): ").strip()
    
    if choice == "2":
        test_single_file()
    elif choice == "3":
        print("Exiting...")
    else:
        main()
