import requests
import base64
import json
import re
import csv
import os
from PIL import Image
from io import BytesIO
from google.oauth2 import service_account
from googleapiclient.discovery import build
from typing import Dict, List, Tuple
import time
import concurrent.futures

# ==============================
#  MODEL SETTINGS
# ==============================
EXTRACTION_MODEL = "gpt-4o"        # OCR extraction
DETECTION_MODEL = "gpt-4o-mini"    # Cheap detection model

EXTRACTION_TEMP = 0.1
DETECTION_TEMP = 0.0

# Progress tracking file
PROGRESS_FILE = "processed_files_part1.csv"


# ========================================
#  MultiQuestionExtractor
# ========================================
class MultiQuestionExtractor:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.openai.com/v1/chat/completions"
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

    # --------------------------
    # IMAGE OPTIMIZATION
    # --------------------------
    def optimize_image(self, image_url, target_width=900):
        """Download + resize + convert ‚Üí base64 JPEG."""
        try:
            response = requests.get(image_url, timeout=30)
            img = Image.open(BytesIO(response.content))

            width, height = img.size
            if width > target_width:
                ratio = target_width / float(width)
                new_height = int(float(height) * ratio)
                img = img.resize((target_width, new_height), Image.Resampling.LANCZOS)

            if img.mode in ('RGBA', 'P'):
                img = img.convert('RGB')

            output = BytesIO()
            img.save(output, format='JPEG', quality=92, optimize=True)
            return base64.b64encode(output.getvalue()).decode('utf-8')

        except Exception as e:
            print(f"[Image optimization failed] {e}")
            return None

    # --------------------------
    # SMART DETECTION - Check if image contains actual questions
    # --------------------------
    def contains_questions(self, image_url: str, file_name: str) -> bool:
        """Quick check if image contains actual questions (not just references)"""
        base64_image = self.optimize_image(image_url)
        if not base64_image:
            return False

        detection_prompt = """
Does this image contain ACTUAL exam questions starting with question numbers (like Q1, Q2, 1., 2.)? 
Look for complete questions, not just references to question numbers.
Answer ONLY "YES" or "NO".
"""

        payload = {
            "model": DETECTION_MODEL,
            "temperature": DETECTION_TEMP,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": detection_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": "low"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 10
        }

        try:
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)
            if response.status_code == 200:
                content = response.json()["choices"][0]["message"]["content"].strip().upper()
                return "YES" in content
            return False
        except Exception as e:
            print(f"Detection failed for {file_name}: {e}")
            return False

    # ----------------------------------------------
    # MAIN EXTRACTION: gpt-4o (OCR + question split)
    # ----------------------------------------------
    def extract_all_questions_from_image(self, image_url: str, file_name: str) -> List[Dict]:

        base64_image = self.optimize_image(image_url)
        if not base64_image:
            return [{"error": "Image processing failed", "file_name": file_name}]

        prompt = """
You are an OCR + parsing expert. Extract ALL questions from the exam image.

RULES:
1. Extract each question individually.
2. Detect question type: "MCQ" or "FILL_IN_BLANK".
3. Extract instructions (text BEFORE the first question OR lines beginning with:
   - Instruction(s)
   - Direction(s)
   - Read the following
   - For questions ... etc.)
4. Extract option text A/B/C/D.
5. If question or options contain images, put "[IMAGE]" as placeholder for each image in the text.
6. Convert math/chem formulas to **Unicode text** (NOT LaTeX).
7. If the question contains a TABLE:
   - Convert the table strictly into valid HTML <table> format.
   - Use only these tags: <table>, <tr>, <th>, <td>.
   - DO NOT return tables as plain text.
   - The table must appear exactly at the same place inside the question.
8. DO NOT wrap the output inside markdown or any emoji.
9. Output must be valid HTML-compatible text only.
10. Output ONLY a JSON ARRAY (no explanation).

JSON SCHEMA EXAMPLE:
[
  {
    "number": "Q1",
    "type": "MCQ",
    "text": "question text [IMAGE] [IMAGE]",
    "option_a": "A text [IMAGE]",
    "option_b": "B text",
    "option_c": "C text [IMAGE]",
    "option_d": "D text",
    "instructions": "common instructions here"
  }
]
"""

        payload = {
            "model": EXTRACTION_MODEL,
            "temperature": EXTRACTION_TEMP,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                                "detail": "high"
                            }
                        }
                    ]
                }
            ],
            "max_tokens": 3500
        }

        try:
            print(f"Extracting questions from: {file_name}")
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=90)

            if response.status_code != 200:
                return [{"error": f"OCR API Error: {response.status_code}", "file_name": file_name}]

            content = response.json()["choices"][0]["message"]["content"].strip()

            # Parse JSON
            questions = self._parse_questions_json(content)
            if not questions:
                print("‚ö† OCR JSON parsing failed ‚Üí fallback text parser")
                return self._fallback_parse(content, file_name)

            # Add defaults with safe type handling
            for q in questions:
                q["file_name"] = file_name
                q.setdefault("type", "MCQ")
                q.setdefault("option_a", "")
                q.setdefault("option_b", "")
                q.setdefault("option_c", "")
                q.setdefault("option_d", "")
                q.setdefault("instructions", "")

            return questions

        except Exception as e:
            return [{"error": f"Extraction failed: {e}", "file_name": file_name}]


    # --------------------------
    # JSON PARSER
    # --------------------------
    def _parse_questions_json(self, text):
        text = text.strip()

        # Direct array
        try:
            obj = json.loads(text)
            if isinstance(obj, list):
                return obj
        except:
            pass

        # Code block
        m = re.search(r'```json\s*(\[.*?\])\s*```', text, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(1))
            except:
                pass

        # None found
        return None


    # -----------------------------------
    # FALLBACK TEXT PARSER (simple regex)
    # -----------------------------------
    def _fallback_parse(self, raw_text, file_name):

        lines = raw_text.split("\n")
        questions = []
        current = None
        instructions = []
        question_found = False

        for line in lines:
            t = line.strip()
            if not t:
                continue

            # instructions block BEFORE first question
            if not question_found and re.match(r'(instruction|direction|read)', t, re.I):
                instructions.append(t)
                continue

            # detect question
            qmatch = re.match(r'(Q\.?\s*\d+|\d+\.)', t, re.I)
            if qmatch:
                question_found = True

                if current:
                    current["instructions"] = " ".join(instructions)
                    questions.append(current)

                num = qmatch.group(1)
                txt = t[len(num):].strip()

                current = {
                    "number": num.replace(".", ""),
                    "type": "MCQ",
                    "text": txt,
                    "option_a": "",
                    "option_b": "",
                    "option_c": "",
                    "option_d": "",
                    "instructions": "",
                    "file_name": file_name
                }
                continue

            # detect options
            if current:
                om = re.match(r'([A-D])[\.\)]\s*(.+)', t)
                if om:
                    key = f"option_{om.group(1).lower()}"
                    current[key] = om.group(2)
                    continue

                # add to question text
                current["text"] += " " + t

        # last question
        if current:
            current["instructions"] = " ".join(instructions)
            questions.append(current)

        return questions

    # -----------------------------------------------
    # PROCESS IMAGES IN BATCHES (OCR ONLY) - MAINTAINS ORDER
    # -----------------------------------------------
    def process_image_batch(self, image_batch: List[Tuple[str, str]]) -> List[Dict]:
        results = []
        
        # First, detect which images contain actual questions
        print("üîç Detecting images with actual questions...")
        valid_images = []
        
        for name, url in image_batch:
            if self.contains_questions(url, name):
                print(f"‚úÖ {name} - Contains questions")
                valid_images.append((name, url))
            else:
                print(f"‚è≠Ô∏è {name} - No questions found (skipping)")
        
        if not valid_images:
            return results
            
        # Process only images with actual questions
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            # Submit all tasks and store them in order
            future_to_index = {
                executor.submit(self.extract_all_questions_from_image, url, name): i 
                for i, (name, url) in enumerate(valid_images)
            }
            
            # Create a list to store results in original order
            temp_results = [None] * len(valid_images)
            
            for future in concurrent.futures.as_completed(future_to_index):
                index = future_to_index[future]
                fname, url = valid_images[index]
                try:
                    res = future.result()
                    temp_results[index] = res
                except Exception as e:
                    temp_results[index] = [{"error": str(e), "file_name": fname}]
            
            # Flatten the results while maintaining order
            for result in temp_results:
                if result is not None:
                    results.extend(result)
        
        return results


# =====================================================
#  GOOGLE SHEET PROCESSOR - PART 1
# =====================================================
class GoogleSheetProcessorPart1:
    def __init__(self, credentials_file: str, sheet_id: str, api_key: str, sheet_tab: str):
        self.sheet_id = sheet_id
        self.sheet_tab = sheet_tab
        self.extractor = MultiQuestionExtractor(api_key)
        self.service = self._auth(credentials_file)

    # ----------------------
    # AUTH
    # ----------------------
    def _auth(self, credentials_file):
        try:
            scopes = ['https://www.googleapis.com/auth/spreadsheets']
            creds = service_account.Credentials.from_service_account_file(
                credentials_file, scopes=scopes)
            return build('sheets', 'v4', credentials=creds)
        except Exception as e:
            print("Google Sheets Auth Failed:", e)
            raise

    # ----------------------
    # READ SHEET
    # ----------------------
    def read_sheet(self):
        try:
            result = self.service.spreadsheets().values().get(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!A:B"  # Only read file names and URLs
            ).execute()

            return result.get("values", [])
        except Exception as e:
            print("Error reading sheet:", e)
            return []

    # ----------------------
    # PROGRESS TRACKING - CSV BASED
    # ----------------------
    def load_processed_files(self):
        """Load already processed files from CSV"""
        processed_files = set()
        if os.path.exists(PROGRESS_FILE):
            try:
                with open(PROGRESS_FILE, 'r', newline='') as f:
                    reader = csv.reader(f)
                    for row in reader:
                        if row:
                            processed_files.add(row[0])
                print(f"üìä Loaded {len(processed_files)} processed files from {PROGRESS_FILE}")
            except Exception as e:
                print(f"Error reading progress file: {e}")
        return processed_files

    def save_processed_files(self, processed_files):
        """Save processed files to CSV"""
        try:
            with open(PROGRESS_FILE, 'w', newline='') as f:
                writer = csv.writer(f)
                for file_name in processed_files:
                    writer.writerow([file_name])
            print(f"üíæ Saved {len(processed_files)} processed files to {PROGRESS_FILE}")
        except Exception as e:
            print(f"Error saving progress file: {e}")

    def mark_file_processed(self, file_name, processed_files):
        """Mark a file as processed and save to CSV"""
        processed_files.add(file_name)
        self.save_processed_files(processed_files)

    # ----------------------
    # CLEAR OUTPUT COLUMNS (OPTIONAL - now only for full reset)
    # ----------------------
    def clear_output(self):
        try:
            # Clear columns C to N (question data columns)
            self.service.spreadsheets().values().clear(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!C:N"
            ).execute()
            print("Old question data cleared (C:N).")
        except Exception as e:
            print("Clear failed:", e)

    # ----------------------
    # UPDATE SHEET
    # ----------------------
    def update_sheet(self, rows: List[List], start_row: int = 2):
        try:
            self.service.spreadsheets().values().update(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!C{start_row}",
                valueInputOption="RAW",
                body={"values": rows}
            ).execute()
            print(f"Sheet updated successfully from row {start_row}.")
        except Exception as e:
            print("Update failed:", e)

    # =====================================================
    # PROCESS ALL IMAGES WITH RESUME CAPABILITY
    # =====================================================
    def process_all_images(self, batch_size=3, resume=True):

        data = self.read_sheet()

        if len(data) < 2:
            print("No data in sheet.")
            return

        # Load processed files from CSV
        processed_files = self.load_processed_files()
        
        if resume and processed_files:
            print(f"üìä Resuming - Found {len(processed_files)} already processed files")
        else:
            print("üÜï Starting fresh - No previous progress found")

        # Don't clear output if resuming and there are processed files
        if not resume or not processed_files:
            self.clear_output()

        # build batches, skip already processed files
        batches = []
        temp = []
        processed_count = 0

        for i, row in enumerate(data[1:], start=2):
            if len(row) < 2:
                continue

            fname = row[0]
            url = row[1]

            if not url.startswith("http"):
                continue

            # Skip already processed files when resuming
            if resume and fname in processed_files:
                processed_count += 1
                continue

            temp.append((fname, url))

            if len(temp) == batch_size:
                batches.append(temp)
                temp = []

        if temp:
            batches.append(temp)

        print(f"üìÅ New files to process: {sum(len(b) for b in batches)}")
        print(f"‚úÖ Already processed: {processed_count}")

        if not batches:
            print("üéâ All files already processed!")
            return

        # process batches
        all_questions = []

        for i, b in enumerate(batches, start=1):
            print(f"\n=== Processing batch {i}/{len(batches)} ===")
            qlist = self.extractor.process_image_batch(b)

            # Mark files as processed
            for q in qlist:
                if "file_name" in q and q["file_name"] not in processed_files:
                    self.mark_file_processed(q["file_name"], processed_files)

            all_questions.extend(qlist)
            time.sleep(3)

        # format sheet rows (only question data, no answers)
        # Columns: C=Type, D=Number, E=Text, F=Option A, G=Option B, H=Option C, I=Option D, 
        #          M=Instructions, N=File Name
        rows = []

        for q in all_questions:
            if "error" in q:
                rows.append([
                    "ERROR",           # C: Type
                    "",                # D: Number  
                    q["error"],        # E: Text
                    "", "", "", "",    # F-I: Options A-D
                    "", "", "", "",   # J-L: (Empty - removed columns)
                    "",               # M: Instructions
                    q.get("file_name","")  # N: File Name
                ])
                continue
            
            rows.append([
                q.get("type",""),      # C: Type
                q.get("number",""),    # D: Number
                q.get("text",""),      # E: Text
                q.get("option_a",""),  # F: Option A
                q.get("option_b",""),  # G: Option B  
                q.get("option_c",""),  # H: Option C
                q.get("option_d",""),  # I: Option D
                "", "", "",           # J-L: (Empty - removed columns)
                q.get("instructions",""),  # M: Instructions
                q.get("file_name","")  # N: File Name
            ])

        # Find the next empty row for appending
        existing_data_rows = len(data) - 1 if len(data) > 1 else 0
        next_row = existing_data_rows + 2  # +2 because header is row 1, data starts at row 2
        
        if rows:
            self.update_sheet(rows, next_row)
            print(f"‚úÖ Successfully updated {len(rows)} question rows to the sheet")
        else:
            print("‚ö† No question rows to update")

# =====================================================
# MAIN RUNNER - PART 1
# =====================================================

def main_part1():

    # ====== IMPORTANT CONFIGURATION ======
    OPENAI_API_KEY = "your_openai_api_key_here"

    if not OPENAI_API_KEY or OPENAI_API_KEY == "your_openai_api_key_here":
        print("‚ùå ERROR: Please put your real OpenAI API key in the script.")
        return

    GOOGLE_SHEET_ID = "1U6gW0yqh3GZlkyxvhF_5k3sXMP8GwZT-TNsXqKv7S1o"
    CREDENTIALS_FILE = "service-account.json"
    SHEET_TAB = "Sheet4"

    BATCH_SIZE = 3            # OCR images per batch
    RESUME_MODE = True        # Set to False to start from scratch

    print("\n" + "="*80)
    print(" üîç PART 1: QUESTION EXTRACTION ONLY ")
    print("="*80)
    print(f" Extraction Model : {EXTRACTION_MODEL}")
    print(f" Detection Model  : {DETECTION_MODEL}")
    print(f" Batch Size       : {BATCH_SIZE}")
    print(f" Resume Mode      : {'ON' if RESUME_MODE else 'OFF'}")
    print(f" Progress File    : {PROGRESS_FILE}")
    print("="*80)

    try:
        processor = GoogleSheetProcessorPart1(
            CREDENTIALS_FILE,
            GOOGLE_SHEET_ID,
            OPENAI_API_KEY,
            SHEET_TAB
        )

        processor.process_all_images(batch_size=BATCH_SIZE, resume=RESUME_MODE)

        print("\nüéâ PART 1 COMPLETED SUCCESSFULLY!")
        print("All questions extracted and saved to sheet columns C through N.")

    except Exception as e:
        print(f"\n‚ùå FATAL ERROR: {e}")
        import traceback
        traceback.print_exc()


# =====================================================
# ENTRY POINT
# =====================================================
if __name__ == "__main__":
    main_part1()
