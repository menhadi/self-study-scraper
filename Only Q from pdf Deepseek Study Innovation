import requests
import json
import re
import os
import time
import csv
import PyPDF2
import pdfplumber
from typing import Dict, List, Tuple, Optional
from google.oauth2 import service_account
from googleapiclient.discovery import build
import io

# ==============================
#  CONFIGURATION
# ==============================
DEEPSEEK_API_KEY = "sk-467f5288c9ef40a4ae6ccec5978019ea"  # REPLACE WITH YOUR KEY
EXTRACTION_MODEL = "deepseek-chat"
BASE_URL = "https://api.deepseek.com/v1/chat/completions"

GOOGLE_SHEET_ID = "1U6gW0yqh3GZlkyxvhF_5k3sXMP8GwZT-TNsXqKv7S1o"
CREDENTIALS_FILE = "service-account.json"
SHEET_TAB = "Sheet4"

PROGRESS_FILE = "processed_pdfs_part1.csv"
UPDATE_BATCH_SIZE = 3
SKIP_PATTERNS = ['cover', 'title_page', 'instruction_only', 'answer_key', 'solution_manual']

# ==============================
#  PDF TEXT EXTRACTOR (IMPROVED)
# ==============================
class PDFTextExtractor:
    def __init__(self):
        pass
    
    def extract_text_from_pdf(self, pdf_bytes: bytes) -> Tuple[str, List[Dict]]:
        """Extract text from digital PDF with page information"""
        text = ""
        page_info = []
        
        try:
            # Try pdfplumber first (better for digital PDFs)
            try:
                with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
                    for page_num, page in enumerate(pdf.pages, 1):
                        try:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                page_text = self._clean_text(page_text)
                                text += f"\n--- Page {page_num} ---\n{page_text}\n"
                                page_info.append({
                                    'page': page_num,
                                    'text': page_text,
                                    'has_questions': self._page_has_questions(page_text),
                                    'char_count': len(page_text)
                                })
                        except Exception as e:
                            error_msg = str(e)[:100]
                            print(f"Warning: Error on page {page_num} (pdfplumber): {error_msg}")
                            continue
            except Exception as e:
                error_msg = str(e)[:100]
                print(f"Warning: pdfplumber failed: {error_msg}")
            
            # If pdfplumber didn't extract enough text, try PyPDF2
            if not text.strip() or len(text.strip()) < 100:
                print("Trying PyPDF2 as fallback...")
                try:
                    reader = PyPDF2.PdfReader(io.BytesIO(pdf_bytes))
                    for page_num, page in enumerate(reader.pages, 1):
                        try:
                            page_text = page.extract_text()
                            if page_text and page_text.strip():
                                page_text = self._clean_text(page_text)
                                text += f"\n--- Page {page_num} ---\n{page_text}\n"
                                page_info.append({
                                    'page': page_num,
                                    'text': page_text,
                                    'has_questions': self._page_has_questions(page_text),
                                    'char_count': len(page_text)
                                })
                        except Exception as e:
                            error_msg = str(e)[:100]
                            print(f"Warning: Error on page {page_num} (PyPDF2): {error_msg}")
                            continue
                except Exception as e:
                    error_msg = str(e)[:100]
                    print(f"Error: PyPDF2 also failed: {error_msg}")
            
            return text, page_info
            
        except Exception as e:
            print(f"Error: PDF extraction error: {str(e)}")
            return "", []
    
    def _clean_text(self, text: str) -> str:
        """Clean extracted PDF text"""
        if not text:
            return ""
        
        # Remove null bytes and special characters
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
        text = re.sub(r'\ufeff', '', text)  # Remove BOM
        text = re.sub(r'�', '', text)  # Remove replacement characters
        
        # Fix common PDF extraction issues
        text = re.sub(r'\s+', ' ', text)  # Replace multiple spaces/newlines with single space
        text = re.sub(r'\s*\.\s*', '. ', text)  # Fix spacing around periods
        text = re.sub(r'\s*,\s*', ', ', text)  # Fix spacing around commas
        
        # Remove page numbers and headers/footers
        text = re.sub(r'Page\s+\d+\s+of\s+\d+', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\d+\s*/\s*\d+', '', text)  # Remove fractions like 1/10
        
        return text.strip()
    
    def _page_has_questions(self, page_text: str) -> bool:
        """Check if page likely contains questions"""
        if not page_text or len(page_text) < 50:
            return False
        
        page_lower = page_text.lower()
        
        # Comprehensive question patterns
        question_indicators = [
            # Numbered questions
            r'q\.?\s*\d+', r'\d+\.\s+[a-z]', r'question\s+\d+', r'\(?\d+\)',
            # Multiple choice indicators
            r'\(a\)', r'\(b\)', r'\(c\)', r'\(d\)',
            r'[a-d]\)\s', r'[a-d]\.\s',
            # Question marks
            r'\?', r'which of the following', r'choose the correct',
            # Fill in blank
            r'blank', r'fill in', r'_____', r'_{3,}',
            # Instructions
            r'instruction:', r'direction:', r'read the following',
            # Exam keywords
            r'multiple choice', r'mcq', r'true or false',
            r'select the', r'choose the', r'what is', r'which is'
        ]
        
        for pattern in question_indicators:
            if re.search(pattern, page_lower, re.IGNORECASE):
                return True
        
        # Check for question-like sentences
        sentences = re.split(r'[.!?]', page_text)
        for sentence in sentences:
            if len(sentence.split()) >= 5 and len(sentence.split()) <= 50:
                words = sentence.lower().split()
                if any(word in ['what', 'which', 'why', 'how', 'when', 'where', 'who'] for word in words[:3]):
                    return True
        
        return False
    
    def get_question_pages(self, page_info: List[Dict]) -> List[int]:
        """Get list of page numbers that likely contain questions"""
        return [page['page'] for page in page_info if page.get('has_questions', False)]

# ==============================
#  DEEPSEEK PDF QUESTION EXTRACTOR (ROBUST)
# ==============================
class DeepSeekPDFQuestionExtractor:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        self.pdf_extractor = PDFTextExtractor()
        self.total_questions = 0
        self.total_pages = 0
    
    def download_pdf(self, pdf_url: str) -> Optional[bytes]:
        """Download PDF from URL with retry logic"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"Downloading PDF (attempt {attempt + 1}/{max_retries})...")
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'application/pdf,text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
                
                response = requests.get(pdf_url, headers=headers, timeout=60, stream=True)
                response.raise_for_status()
                
                # Check if it's a PDF
                content_type = response.headers.get('content-type', '').lower()
                if 'pdf' not in content_type and 'octet-stream' not in content_type:
                    print(f"Warning: Content-Type is {content_type}, not PDF")
                
                pdf_bytes = response.content
                
                if len(pdf_bytes) < 100:
                    print(f"Warning: PDF is very small ({len(pdf_bytes)} bytes)")
                    return None
                
                print(f"Downloaded {len(pdf_bytes):,} bytes")
                return pdf_bytes
                
            except requests.exceptions.Timeout:
                print(f"Timeout on attempt {attempt + 1}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    print("All download attempts failed due to timeout")
                    return None
            except requests.exceptions.RequestException as e:
                error_msg = str(e)[:200]
                print(f"Download error: {error_msg}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    return None
            except Exception as e:
                print(f"Unexpected error: {str(e)}")
                return None
        
        return None
    
    def contains_questions(self, pdf_bytes: bytes, file_name: str) -> bool:
        """Detect if PDF contains exam questions"""
        print(f"Checking if PDF contains questions...")
        
        # Quick filename check
        file_lower = file_name.lower()
        if any(pattern in file_lower for pattern in SKIP_PATTERNS):
            print(f"Skipped by filename pattern")
            return False
        
        try:
            # Extract text from first few pages only (for speed)
            text, page_info = self.pdf_extractor.extract_text_from_pdf(pdf_bytes)
            
            if not text or len(text.strip()) < 100:
                print(f"PDF appears empty or unreadable")
                return False
            
            # Check text statistics
            words = len(text.split())
            print(f"PDF contains {words:,} words, {len(page_info)} pages")
            
            # Look for question pages
            question_pages = self.pdf_extractor.get_question_pages(page_info)
            
            if question_pages:
                print(f"Found {len(question_pages)} pages with question patterns")
                return True
            else:
                # Use DeepSeek for final verification
                print(f"Using DeepSeek to verify...")
                return self._deepseek_detection(text[:3000], file_name)
                
        except Exception as e:
            error_msg = str(e)[:200]
            print(f"Detection error: {error_msg}")
            return True  # When in doubt, process it
    
    def _deepseek_detection(self, text_sample: str, file_name: str) -> bool:
        """Use DeepSeek to detect if text contains questions"""
        prompt = f"""Analyze this text sample from "{file_name}". Does it contain ANY exam questions?

Look for:
1. Numbered questions: Q1, 1., Question 1, etc.
2. Multiple choice options: (A), (B), (C), (D) or A), B), C), D)
3. Questions ending with "?"
4. Fill-in-blank patterns: "_____", "blank", "Fill in"
5. Instructions for answering questions
6. Mathematical problems or equations
7. Chemistry/science formulas or reactions

If you see ANY question patterns or exam content, answer YES.
If it's only a cover page, table of contents, or blank, answer NO.

Text sample:
{text_sample[:2500]}

Answer ONLY "YES" or "NO".
"""
        
        payload = {
            "model": EXTRACTION_MODEL,
            "temperature": 0.0,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 10
        }
        
        try:
            response = requests.post(BASE_URL, headers=self.headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
                if isinstance(content, str):
                    content = content.strip().upper()
                    result = "YES" in content
                    print(f"DeepSeek detection: {'QUESTIONS FOUND' if result else 'NO QUESTIONS'}")
                    return result
            return True  # Default to processing if API fails
        except Exception as e:
            error_msg = str(e)[:100]
            print(f"Detection API error: {error_msg}")
            return True
    
    def _convert_latex_to_unicode(self, text: str) -> str:
        """Convert LaTeX notation to Unicode characters"""
        if not text:
            return text
        
        # Comprehensive LaTeX to Unicode mapping
        latex_map = {
            # Subscripts
            r'_{0}': '₀', r'_0': '₀', r'_{1}': '₁', r'_1': '₁',
            r'_{2}': '₂', r'_2': '₂', r'_{3}': '₃', r'_3': '₃',
            r'_{4}': '₄', r'_4': '₄', r'_{5}': '₅', r'_5': '₅',
            r'_{6}': '₆', r'_6': '₆', r'_{7}': '₇', r'_7': '₇',
            r'_{8}': '₈', r'_8': '₈', r'_{9}': '₉', r'_9': '₉',
            r'_{10}': '₁₀', r'_{11}': '₁₁', r'_{12}': '₁₂',
            
            # Superscripts
            r'^{0}': '⁰', r'^0': '⁰', r'^{1}': '¹', r'^1': '¹',
            r'^{2}': '²', r'^2': '²', r'^{3}': '³', r'^3': '³',
            r'^{4}': '⁴', r'^4': '⁴', r'^{5}': '⁵', r'^5': '⁵',
            r'^{6}': '⁶', r'^6': '⁶', r'^{7}': '⁷', r'^7': '⁷',
            r'^{8}': '⁸', r'^8': '⁸', r'^{9}': '⁹', r'^9': '⁹',
            r'^{+}': '⁺', r'^+': '⁺', r'^{-}': '⁻', r'^-': '⁻',
            
            # Greek letters
            r'\\alpha': 'α', r'\\beta': 'β', r'\\gamma': 'γ', r'\\Gamma': 'Γ',
            r'\\delta': 'δ', r'\\Delta': 'Δ', r'\\epsilon': 'ε', r'\\varepsilon': 'ε',
            r'\\zeta': 'ζ', r'\\eta': 'η', r'\\theta': 'θ', r'\\Theta': 'Θ',
            r'\\iota': 'ι', r'\\kappa': 'κ', r'\\lambda': 'λ', r'\\Lambda': 'Λ',
            r'\\mu': 'μ', r'\\nu': 'ν', r'\\xi': 'ξ', r'\\Xi': 'Ξ',
            r'\\pi': 'π', r'\\Pi': 'Π', r'\\rho': 'ρ', r'\\sigma': 'σ',
            r'\\Sigma': 'Σ', r'\\tau': 'τ', r'\\upsilon': 'υ', r'\\Upsilon': 'Υ',
            r'\\phi': 'φ', r'\\Phi': 'Φ', r'\\chi': 'χ', r'\\psi': 'ψ',
            r'\\Psi': 'Ψ', r'\\omega': 'ω', r'\\Omega': 'Ω',
            
            # Math operators
            r'\\times': '×', r'\\div': '÷', r'\\pm': '±', r'\\mp': '∓',
            r'\\cdot': '·', r'\\ast': '*', r'\\star': '⋆',
            r'\\leq': '≤', r'\\geq': '≥', r'\\neq': '≠', r'\\approx': '≈',
            r'\\sim': '∼', r'\\propto': '∝', r'\\infty': '∞',
            r'\\partial': '∂', r'\\nabla': '∇', r'\\forall': '∀',
            r'\\exists': '∃', r'\\in': '∈', r'\\notin': '∉',
            r'\\subset': '⊂', r'\\subseteq': '⊆', r'\\supset': '⊃',
            r'\\supseteq': '⊇', r'\\cup': '∪', r'\\cap': '∩',
            r'\\wedge': '∧', r'\\vee': '∨', r'\\neg': '¬',
            
            # Arrows
            r'\\rightarrow': '→', r'\\Rightarrow': '⇒', r'\\longrightarrow': '⟶',
            r'\\leftarrow': '←', r'\\Leftarrow': '⇐', r'\\longleftarrow': '⟵',
            r'\\leftrightarrow': '↔', r'\\Leftrightarrow': '⇔', r'\\longleftrightarrow': '⟷',
            r'\\mapsto': '↦', r'\\to': '→', r'\\gets': '←',
            
            # Chemical notation
            r'->': '→', r'<->': '↔', r'<=>': '⇌',
            r'\\ce{': '', r'\\chem{': '',  # Remove chemistry package commands
            
            # Common functions
            r'\\sin': 'sin', r'\\cos': 'cos', r'\\tan': 'tan',
            r'\\log': 'log', r'\\ln': 'ln', r'\\exp': 'exp',
            r'\\lim': 'lim', r'\\sum': '∑', r'\\prod': '∏',
            r'\\int': '∫', r'\\oint': '∮',
            
            # Fractions
            r'\\frac{1}{2}': '½', r'\\frac{1}{3}': '⅓', r'\\frac{2}{3}': '⅔',
            r'\\frac{1}{4}': '¼', r'\\frac{3}{4}': '¾', r'\\frac{1}{5}': '⅕',
            r'\\frac{2}{5}': '⅖', r'\\frac{3}{5}': '⅗', r'\\frac{4}{5}': '⅘',
            
            # Other symbols
            r'\\circ': '°', r'\\degree': '°', r'\\angstrom': 'Å',
            r'\\hbar': 'ħ', r'\\ell': 'ℓ',
        }
        
        # Apply direct substitutions
        converted = text
        for latex, unicode_char in latex_map.items():
            converted = converted.replace(latex, unicode_char)
        
        # Handle generic subscripts: H_2O → H₂O, CH_3 → CH₃
        def replace_subscript(match):
            num = match.group(1)
            subscript_map = {'0': '₀', '1': '₁', '2': '₂', '3': '₃', '4': '₄', 
                           '5': '₅', '6': '₆', '7': '₇', '8': '₈', '9': '₉'}
            return ''.join(subscript_map.get(d, d) for d in num)
        
        converted = re.sub(r'_\{(\d+)\}', replace_subscript, converted)
        converted = re.sub(r'_(\d+)', replace_subscript, converted)
        
        # Handle generic superscripts: x^2 → x²
        def replace_superscript(match):
            num = match.group(1)
            superscript_map = {'0': '⁰', '1': '¹', '2': '²', '3': '³', '4': '⁴',
                             '5': '⁵', '6': '⁶', '7': '⁷', '8': '⁸', '9': '⁹',
                             '+': '⁺', '-': '⁻'}
            return ''.join(superscript_map.get(d, d) for d in num)
        
        converted = re.sub(r'\^\{([^}]+)\}', replace_superscript, converted)
        converted = re.sub(r'\^([^\\s{]+)', replace_superscript, converted)
        
        # Remove any remaining LaTeX commands
        converted = re.sub(r'\\[a-zA-Z]+\{.*?\}', '', converted)
        converted = re.sub(r'\\[a-zA-Z]+', '', converted)
        converted = re.sub(r'\{|\}', '', converted)
        
        return converted
    
    def extract_questions_from_pdf(self, pdf_bytes: bytes, file_name: str) -> List[Dict]:
        """Extract questions from PDF using DeepSeek"""
        print(f"\nExtracting questions from: {file_name}")
        
        # Extract text from PDF
        text, page_info = self.pdf_extractor.extract_text_from_pdf(pdf_bytes)
        self.total_pages += len(page_info)
        
        if not text or len(text.strip()) < 200:
            print(f"Insufficient text extracted")
            return [{"error": "Insufficient text extracted from PDF", "file_name": file_name}]
        
        print(f"Extracted {len(text):,} characters, {len(text.split()):,} words")
        
        # Save extracted text for debugging
        safe_filename = file_name.replace('/', '_').replace('\\', '_')
        debug_file = f"debug_{safe_filename}_text.txt"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(text[:10000])  # First 10K chars
        print(f"Saved extracted text to: {debug_file}")
        
        # Get pages with questions
        question_pages = self.pdf_extractor.get_question_pages(page_info)
        if not question_pages:
            print(f"No question pages detected, analyzing first 10 pages")
            question_pages = list(range(1, min(11, len(page_info) + 1)))
        
        print(f"Processing {len(question_pages)} pages for questions")
        
        # Use improved chunking method for multi-page questions
        chunks = self._split_text_into_contextual_chunks(text, page_info, question_pages)
        print(f"Split into {len(chunks)} contextual chunks")
        
        # Process each chunk with DeepSeek
        all_questions = []
        for chunk_num, (chunk_text, pages) in enumerate(chunks, 1):
            print(f"\nProcessing chunk {chunk_num}/{len(chunks)} (pages {pages})")
            
            questions = self._process_text_chunk_with_deepseek(chunk_text, file_name, chunk_num, pages)
            if questions:
                all_questions.extend(questions)
                print(f"Found {len(questions)} questions in this chunk")
            
            # Rate limiting
            time.sleep(2)
        
        # Process and clean the questions
        processed_questions = self._process_extracted_questions(all_questions, file_name)
        
        self.total_questions += len(processed_questions)
        print(f"\nTotal extracted from {file_name}: {len(processed_questions)} questions")
        
        return processed_questions
    
    def _split_text_into_contextual_chunks(self, text: str, page_info: List[Dict], question_pages: List[int]) -> List[Tuple[str, List[int]]]:
        """Split text into contextual chunks that keep multi-page questions together"""
        chunks = []
        
        # Find page markers
        page_markers = re.finditer(r'--- Page (\d+) ---', text)
        page_positions = []
        
        for match in page_markers:
            page_num = int(match.group(1))
            position = match.start()
            page_positions.append((page_num, position))
        
        if not page_positions:
            # If no page markers found, use simple splitting
            chunk_size = 4000
            for i in range(0, len(text), chunk_size):
                chunks.append((text[i:i+chunk_size], [1]))
            return chunks
        
        # Group consecutive question pages
        page_groups = []
        current_group = []
        
        for page in question_pages[:30]:  # Limit to first 30 pages
            if not current_group:
                current_group.append(page)
            elif page == current_group[-1] + 1:
                current_group.append(page)  # Continue group
            else:
                if current_group:
                    page_groups.append(current_group.copy())
                current_group = [page]
        
        if current_group:
            page_groups.append(current_group)
        
        print(f"Grouped into {len(page_groups)} page groups: {page_groups}")
        
        # Create chunks from page groups
        for group in page_groups:
            chunk_text = ""
            chunk_pages = []
            
            for page_num in group:
                # Find this page in the text
                for i, (p_num, position) in enumerate(page_positions):
                    if p_num == page_num:
                        # Get page text
                        start_pos = position
                        if i + 1 < len(page_positions):
                            end_pos = page_positions[i + 1][1]
                        else:
                            end_pos = len(text)
                        
                        page_text = text[start_pos:end_pos]
                        
                        # Add to chunk
                        if chunk_text:
                            chunk_text += "\n" + page_text
                        else:
                            chunk_text = page_text
                        chunk_pages.append(page_num)
                        break
            
            if chunk_text and len(chunk_text) > 100:
                # If chunk is too large, split it
                if len(chunk_text) > 4000:
                    split_chunks = self._split_large_chunk(chunk_text, chunk_pages)
                    chunks.extend(split_chunks)
                else:
                    chunks.append((chunk_text, chunk_pages.copy()))
        
        return chunks
    
    def _split_large_chunk(self, chunk_text: str, chunk_pages: List[int]) -> List[Tuple[str, List[int]]]:
        """Split a large chunk into smaller pieces"""
        chunks = []
        current_pos = 0
        chunk_size = 4000
        
        while current_pos < len(chunk_text):
            end_pos = current_pos + chunk_size
            
            # Try to split at a reasonable boundary
            if end_pos < len(chunk_text):
                # Look for question boundary
                boundary = chunk_text.rfind('Q', current_pos, end_pos)
                if boundary > current_pos + chunk_size * 0.7:  # If found near the end
                    end_pos = boundary
            
            chunks.append((chunk_text[current_pos:end_pos], chunk_pages.copy()))
            current_pos = end_pos
        
        return chunks
    
    def _process_text_chunk_with_deepseek(self, chunk_text: str, file_name: str, 
                                         chunk_num: int, pages: List[int]) -> List[Dict]:
        """Call DeepSeek API to extract questions from text chunk"""
        prompt = f"""You are an expert at extracting structured exam questions from text. Extract ALL questions from the following text.

CRITICAL INSTRUCTIONS:
1. Extract ALL questions you find
2. Handle multi-page questions - if a question continues across pages, keep it together
3. For each question, identify:
   - Question number (extract from text like Q98, Q121, 98., 121., etc.)
   - Question type: "MCQ" for multiple choice, "FILL_IN_BLANK" for fill-in-blank
   - Question text (the actual question)
   - Options A, B, C, D (for MCQ)
   - Any instructions before the question
   - Answer (from "Ans:" or "Answer:" section)
   - Explanation (from "Explanation:" section)

IMPORTANT DISTINCTIONS:
1. Numbers like 1., 2., 3., 4. INSIDE the question text are PART OF THE QUESTION, not options
2. ONLY (i), (ii), (iii), (iv) or A), B), C), D) or (A), (B), (C), (D) are OPTIONS
3. Example: If question has "1. It turns lime water milky. 2. It extinguishes a burning splinter." 
   These are PART OF THE QUESTION STATEMENT, not options
4. Options are typically labeled with letters: A, B, C, D or (i), (ii), (iii), (iv)

5. Convert ALL LaTeX to Unicode: H_2O → H₂O, \\alpha → α, \\rightarrow → →
6. For chemical structures or complex diagrams in options → use "[DIAGRAM]"
7. Preserve tables as HTML if present
8. If you can't determine question number, use "Q{chunk_num}.{chunk_num}"
9. Output MUST be a valid JSON array

TEXT FROM "{file_name}" (Pages {pages}):
{chunk_text[:3500]}

RESPONSE FORMAT (JSON array):
[
  {{
    "number": "Q98",
    "type": "MCQ",
    "text": "Sodium hydrogen carbonate when added to acetic acid evolves a gas. Which of the following statements are true about the gas evolved? 1. It turns lime water milky. 2. It extinguishes a burning splinter. 3. It dissolves in a solution of sodium hydro. 4. It has a pungent odour.",
    "option_a": "(i) and (ii)",
    "option_b": "(i), (ii) and (iii)", 
    "option_c": "(ii), (iii) and (iv)",
    "option_d": "(i) and (iv)",
    "instructions": "",
    "answer": "B. (i), (ii) and (iii)",
    "explanation": "Reaction of sodium hydrogen carbonate with acetic acid forms sodium acetate and water with carbon dioxide CO₂ gas. NaHCO₃ + CH₃COOH → CH₃COONa + CO₂ + H₂O"
  }},
  {{
    "number": "Q121",
    "type": "MCQ",
    "text": "In one of the industrial processes used for manufacture of sodium hydroxide, a gas X is formed as by-product. The gas X reacts with lime water to give a compound Y which is used as a bleaching agent in chemical industry. The compound X and Y could be:",
    "option_a": "H₂ and NaHCO₃, respectively.",
    "option_b": "CO₂ and CaOCl₂ respectively.",
    "option_c": "Cl₂ and CaOCl₂ respectively.", 
    "option_d": "Cl₂ and NaHCO₃ respectively.",
    "instructions": "",
    "answer": "C. Cl₂ and CaOCl₂ respectively.",
    "explanation": ""
  }}
]
"""
        
        payload = {
            "model": EXTRACTION_MODEL,
            "temperature": 0.1,
            "messages": [
                {
                    "role": "system",
                    "content": "You are an expert exam question extractor. Handle multi-page questions. Distinguish between numbered statements in question text and actual options. Always respond with valid JSON arrays."
                },
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 4000
        }
        
        try:
            print(f"Calling DeepSeek API...")
            start_time = time.time()
            response = requests.post(BASE_URL, headers=self.headers, json=payload, timeout=120)
            api_time = time.time() - start_time
            
            if response.status_code != 200:
                error_msg = f"API Error {response.status_code}"
                print(f"{error_msg}: {response.text[:200]}")
                return []
            
            data = response.json()
            content = data.get("choices", [{}])[0].get("message", {}).get("content", "")
            
            if not content or not isinstance(content, str):
                print("Empty or invalid response from API")
                return []
            
            print(f"API response received in {api_time:.1f}s, {len(content)} characters")
            
            # Try to parse JSON
            questions = self._parse_json_response(content)
            if questions:
                return questions
            else:
                print("JSON parsing failed, trying fallback parser...")
                return self._fallback_parse_improved(content, file_name, chunk_num, chunk_text)
                
        except requests.exceptions.Timeout:
            print("API timeout")
            return []
        except Exception as e:
            error_msg = str(e)[:200]
            print(f"API error: {error_msg}")
            return []
    
    def _parse_json_response(self, text: str) -> List[Dict]:
        """Parse JSON from DeepSeek response with multiple fallbacks"""
        text = text.strip()
        
        # Clean the text
        text = re.sub(r'```json|```', '', text)
        text = re.sub(r'^\s*json\s*', '', text, flags=re.IGNORECASE)
        
        # Try different JSON extraction methods
        extraction_methods = [
            # Method 1: Direct JSON parse
            lambda t: json.loads(t) if t.startswith('[') and t.endswith(']') else None,
            
            # Method 2: Find JSON array in text
            lambda t: (re.search(r'(\[\s*\{.*?\}\s*\])', t, re.DOTALL) and 
                      json.loads(re.search(r'(\[\s*\{.*?\}\s*\])', t, re.DOTALL).group(1))),
            
            # Method 3: Multiple JSON objects
            lambda t: (json.loads('[' + ','.join(re.findall(r'\{.*?\}', t, re.DOTALL)) + ']') 
                      if re.findall(r'\{.*?\}', t, re.DOTALL) else None),
            
            # Method 4: Try with error recovery
            lambda t: self._try_json_with_error_recovery(t)
        ]
        
        for method in extraction_methods:
            try:
                result = method(text)
                if result and isinstance(result, list):
                    print(f"JSON parsed successfully: {len(result)} items")
                    return result
            except:
                continue
        
        return []
    
    def _try_json_with_error_recovery(self, text: str):
        """Try to parse JSON with error recovery"""
        try:
            # Try to fix common JSON issues
            # Remove trailing commas
            text = re.sub(r',\s*}', '}', text)
            text = re.sub(r',\s*]', ']', text)
            
            # Fix missing quotes
            text = re.sub(r'(\w+)\s*:', r'"\1":', text)
            
            # Fix single quotes to double quotes
            text = text.replace("'", '"')
            
            return json.loads(text)
        except:
            return None
    
    def _fallback_parse_improved(self, text: str, file_name: str, chunk_num: int, original_text: str) -> List[Dict]:
        """Improved fallback parser for complex question formats"""
        print(f"Using improved fallback parser for chunk {chunk_num}")
        
        questions = []
        lines = text.split('\n')
        current_question = None
        question_counter = 0
        in_question_text = False
        collecting_options = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # Look for question start - more comprehensive patterns
            question_match = re.match(
                r'(Q\.?\s*\d+|Question\s+\d+|\d+\.\s+[A-Z]|\d+\)\s+[A-Z]|\(\d+\)\s+[A-Z]|Q\d+[\.:])', 
                line, 
                re.IGNORECASE
            )
            
            if question_match:
                question_counter += 1
                
                if current_question:
                    questions.append(current_question)
                
                # Get question number
                q_num_match = re.search(r'(\d+)', question_match.group(0))
                q_number = f"Q{q_num_match.group(1)}" if q_num_match else f"Q{chunk_num}.{question_counter}"
                
                current_question = {
                    "number": q_number,
                    "type": "MCQ",
                    "text": line[len(question_match.group(0)):].strip(),
                    "option_a": "",
                    "option_b": "",
                    "option_c": "",
                    "option_d": "",
                    "instructions": "",
                    "answer": "",
                    "explanation": "",
                    "file_name": file_name
                }
                in_question_text = True
                collecting_options = False
                continue
            
            # Look for answer section
            if current_question and not current_question.get("answer"):
                answer_match = re.match(r'^\s*(Ans:|Answer:|Answer\s*:)\s*(.+)', line, re.IGNORECASE)
                if answer_match:
                    current_question["answer"] = answer_match.group(2).strip()
                    in_question_text = False
                    collecting_options = False
                    continue
            
            # Look for explanation section
            if current_question and not current_question.get("explanation"):
                explanation_match = re.match(r'^\s*(Explanation:|Explanation\s*:)\s*(.+)', line, re.IGNORECASE)
                if explanation_match:
                    current_question["explanation"] = explanation_match.group(2).strip()
                    in_question_text = False
                    collecting_options = False
                    continue
            
            # Look for options - only A-D or i-iv
            if current_question:
                # Check for letter options (A, B, C, D)
                option_match = re.match(r'^\s*([A-D])[\.\)]\s*(.+)', line)
                if option_match:
                    option_key = f"option_{option_match.group(1).lower()}"
                    current_question[option_key] = option_match.group(2).strip()
                    collecting_options = True
                    in_question_text = False
                    continue
                
                # Check for Roman numeral options (i, ii, iii, iv)
                option_match2 = re.match(r'^\s*(\(?i{1,3}v?\)?)\s*(.+)', line, re.IGNORECASE)
                if option_match2 and collecting_options:
                    # Map i, ii, iii, iv to a, b, c, d
                    roman_to_letter = {'i': 'a', 'ii': 'b', 'iii': 'c', 'iv': 'd', 
                                      '(i)': 'a', '(ii)': 'b', '(iii)': 'c', '(iv)': 'd'}
                    roman_key = option_match2.group(1).lower()
                    if roman_key in roman_to_letter:
                        option_key = f"option_{roman_to_letter[roman_key]}"
                        current_question[option_key] = option_match2.group(2).strip()
                    continue
                
                # Check for parenthesized letter options
                option_match3 = re.match(r'^\s*\(([A-D])\)\s*(.+)', line)
                if option_match3:
                    option_key = f"option_{option_match3.group(1).lower()}"
                    current_question[option_key] = option_match3.group(2).strip()
                    collecting_options = True
                    in_question_text = False
                    continue
            
            # Add to question text if we're still in question text (not collecting options)
            if current_question and in_question_text and not collecting_options:
                # Check if this line might start options
                if re.match(r'^\s*[A-D][\.\)]', line) or re.match(r'^\s*\([A-D]\)', line):
                    collecting_options = True
                    in_question_text = False
                elif len(current_question["text"]) < 1000:  # Limit question text length
                    current_question["text"] += " " + line
        
        # Add the last question
        if current_question:
            questions.append(current_question)
        
        # If no questions found, try alternative parsing
        if not questions:
            questions = self._alternative_question_detection(original_text, file_name, chunk_num)
        
        return questions
    
    def _alternative_question_detection(self, text: str, file_name: str, chunk_num: int) -> List[Dict]:
        """Alternative method for detecting questions in text"""
        questions = []
        
        # Look for question patterns in the original text
        question_patterns = [
            r'(Q\d+[\.:]?\s+.+?)(?=Q\d+|Ans:|Explanation:|$)',
            r'(\d+\.\s+[A-Z].+?)(?=\d+\.\s+[A-Z]|Ans:|Explanation:|$)',
            r'(Question\s+\d+[\.:]?.+?)(?=Question\s+\d+|Ans:|Explanation:|$)'
        ]
        
        for pattern in question_patterns:
            matches = re.finditer(pattern, text, re.DOTALL | re.IGNORECASE)
            for i, match in enumerate(matches):
                question_text = match.group(1).strip()
                
                # Extract question number
                q_num_match = re.search(r'(\d+)', question_text[:50])
                q_number = f"Q{q_num_match.group(1)}" if q_num_match else f"Q{chunk_num}.{i+1}"
                
                # Look for options in the question text
                options = self._extract_options_from_text(question_text)
                
                question = {
                    "number": q_number,
                    "type": "MCQ",
                    "text": question_text,
                    "option_a": options.get('a', ''),
                    "option_b": options.get('b', ''),
                    "option_c": options.get('c', ''),
                    "option_d": options.get('d', ''),
                    "instructions": "",
                    "answer": "",
                    "explanation": "",
                    "file_name": file_name
                }
                
                # Look for answer and explanation after the question
                after_text = text[match.end():match.end()+500]
                answer_match = re.search(r'Ans[\.:]?\s*(.+)', after_text, re.IGNORECASE)
                if answer_match:
                    question["answer"] = answer_match.group(1).strip()[:200]
                
                explanation_match = re.search(r'Explanation[\.:]?\s*(.+)', after_text, re.IGNORECASE)
                if explanation_match:
                    question["explanation"] = explanation_match.group(1).strip()[:500]
                
                questions.append(question)
        
        return questions
    
    def _extract_options_from_text(self, text: str) -> Dict[str, str]:
        """Extract options from question text"""
        options = {}
        
        # Look for A, B, C, D options
        option_patterns = [
            r'([A-D])[\.\)]\s*(.+?)(?=[A-D][\.\)]|$)',
            r'\(([A-D])\)\s*(.+?)(?=\([A-D]\)|$)',
            r'([A-D]\.)\s*(.+?)(?=[A-D]\.|$)'
        ]
        
        for pattern in option_patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for letter, option_text in matches:
                key = letter.lower()
                options[key] = option_text.strip()
        
        # If no letter options, look for Roman numeral options
        if not options:
            roman_pattern = r'(\(?i{1,3}v?\)?)\s*(.+?)(?=\(?i{1,3}v?\)?|$)'
            roman_matches = re.findall(roman_pattern, text, re.IGNORECASE | re.DOTALL)
            roman_map = {'i': 'a', 'ii': 'b', 'iii': 'c', 'iv': 'd', 
                        '(i)': 'a', '(ii)': 'b', '(iii)': 'c', '(iv)': 'd'}
            
            for roman, option_text in roman_matches[:4]:  # Limit to 4 options
                key = roman_map.get(roman.lower(), '')
                if key and key not in options:
                    options[key] = option_text.strip()
        
        return options
    
    def _process_extracted_questions(self, questions: List[Dict], file_name: str) -> List[Dict]:
        """Process and clean extracted questions"""
        processed = []
        
        for i, q in enumerate(questions):
            if not isinstance(q, dict):
                continue
            
            # Ensure required fields exist
            q.setdefault("number", f"Q{i+1}")
            q.setdefault("type", "MCQ")
            q.setdefault("text", "")
            q.setdefault("option_a", "")
            q.setdefault("option_b", "")
            q.setdefault("option_c", "")
            q.setdefault("option_d", "")
            q.setdefault("instructions", "")
            q.setdefault("answer", "")
            q.setdefault("explanation", "")
            q.setdefault("file_name", file_name)
            
            # Convert LaTeX to Unicode in all text fields
            for field in ["text", "option_a", "option_b", "option_c", "option_d", 
                         "instructions", "answer", "explanation"]:
                if field in q and q[field]:
                    q[field] = self._convert_latex_to_unicode(q[field])
            
            # Clean text - remove extra whitespace
            for field in ["text", "option_a", "option_b", "option_c", "option_d", 
                         "answer", "explanation"]:
                if field in q and q[field]:
                    q[field] = re.sub(r'\s+', ' ', q[field]).strip()
            
            # Determine question type
            if q["type"] not in ["MCQ", "FILL_IN_BLANK", "MATCHING"]:
                if any(q[f"option_{opt}"] for opt in ['a', 'b', 'c', 'd']):
                    q["type"] = "MCQ"
                elif re.search(r'_{3,}|_____|blank', q["text"], re.IGNORECASE):
                    q["type"] = "FILL_IN_BLANK"
                else:
                    q["type"] = "MCQ"
            
            # Clean answer field
            if q.get("answer"):
                q["answer"] = re.sub(r'^Ans(?:wer)?\s*[:\-\.]\s*', '', q["answer"], flags=re.IGNORECASE)
            
            # Clean explanation field
            if q.get("explanation"):
                q["explanation"] = re.sub(r'^Explanation\s*[:\-\.]\s*', '', q["explanation"], flags=re.IGNORECASE)
            
            # Truncate very long fields
            for field in ["text", "explanation"]:
                if field in q and q[field] and len(q[field]) > 2000:
                    q[field] = q[field][:2000] + "..."
            
            processed.append(q)
        
        return processed
    
    def process_pdf_batch(self, pdf_batch: List[Tuple[str, str]]) -> List[Dict]:
        """Process a batch of PDFs"""
        results = []
        
        for file_name, pdf_url in pdf_batch:
            print(f"\n{'='*60}")
            print(f"PROCESSING: {file_name}")
            print(f"URL: {pdf_url[:100]}...")
            print(f"{'='*60}")
            
            try:
                # Download PDF
                pdf_bytes = self.download_pdf(pdf_url)
                if not pdf_bytes:
                    results.append({"error": "Failed to download PDF", "file_name": file_name})
                    continue
                
                # Check if PDF contains questions
                if not self.contains_questions(pdf_bytes, file_name):
                    print(f"Skipping {file_name} - no questions found")
                    results.append({"info": "No questions found", "file_name": file_name})
                    continue
                
                # Extract questions
                questions = self.extract_questions_from_pdf(pdf_bytes, file_name)
                results.extend(questions)
                
                # Rate limiting between PDFs
                time.sleep(3)
                
            except Exception as e:
                error_msg = f"Error processing {file_name}: {str(e)[:200]}"
                print(f"{error_msg}")
                results.append({"error": error_msg, "file_name": file_name})
        
        return results
    
    def print_statistics(self):
        """Print processing statistics"""
        print(f"\n{'='*60}")
        print("PROCESSING STATISTICS")
        print(f"{'='*60}")
        print(f"Total questions extracted: {self.total_questions}")
        print(f"Total pages processed: {self.total_pages}")
        if self.total_pages > 0:
            print(f"Average questions per page: {self.total_questions/self.total_pages:.2f}")

# ==============================
#  GOOGLE SHEETS PROCESSOR
# ==============================
class GoogleSheetsPDFProcessor:
    def __init__(self, credentials_file: str, sheet_id: str, api_key: str, sheet_tab: str):
        self.sheet_id = sheet_id
        self.sheet_tab = sheet_tab
        self.extractor = DeepSeekPDFQuestionExtractor(api_key)
        self.service = self._authenticate(credentials_file)
        self.next_output_row = 2
    
    def _authenticate(self, credentials_file: str):
        """Authenticate with Google Sheets API"""
        try:
            if not os.path.exists(credentials_file):
                raise FileNotFoundError(f"Credentials file not found: {credentials_file}")
            
            scopes = ['https://www.googleapis.com/auth/spreadsheets']
            creds = service_account.Credentials.from_service_account_file(
                credentials_file, scopes=scopes)
            return build('sheets', 'v4', credentials=creds)
        except Exception as e:
            print(f"Google Sheets authentication failed: {e}")
            raise
    
    def read_sheet_data(self) -> List[List[str]]:
        """Read file names and URLs from sheet"""
        try:
            result = self.service.spreadsheets().values().get(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!A:B"
            ).execute()
            
            values = result.get('values', [])
            print(f"Read {len(values)} rows from sheet")
            return values
        except Exception as e:
            print(f"Error reading sheet: {e}")
            return []
    
    def load_processed_files(self) -> set:
        """Load already processed files from CSV"""
        processed_files = set()
        
        if os.path.exists(PROGRESS_FILE):
            try:
                with open(PROGRESS_FILE, 'r', newline='', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    for row in reader:
                        if row and row[0]:
                            processed_files.add(row[0])
                print(f"Loaded {len(processed_files)} processed files from {PROGRESS_FILE}")
            except Exception as e:
                print(f"Error reading progress file: {e}")
        
        return processed_files
    
    def save_processed_files(self, processed_files: set):
        """Save processed files to CSV"""
        try:
            with open(PROGRESS_FILE, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                for file_name in sorted(processed_files):
                    writer.writerow([file_name])
            print(f"Saved {len(processed_files)} processed files")
        except Exception as e:
            print(f"Error saving progress file: {e}")
    
    def clear_output_columns(self):
        """Clear columns C through M"""
        try:
            self.service.spreadsheets().values().clear(
                spreadsheetId=self.sheet_id,
                range=f"{self.sheet_tab}!C:M"
            ).execute()
            print("Cleared previous output columns (C-M)")
        except Exception as e:
            print(f"Error clearing columns: {e}")
    
    def update_sheet_with_questions(self, questions_batch: List[Dict]):
        """Update Google Sheets with extracted questions including answer and explanation"""
        if not questions_batch:
            return
        
        rows = []
        for q in questions_batch:
            if not isinstance(q, dict):
                continue
            
            if "error" in q:
                rows.append([
                    "ERROR",                    # C: Type
                    "",                         # D: Number
                    q.get("file_name", ""),     # E: File Name
                    "",                         # F: Instructions
                    q.get("error", ""),         # G: Text
                    "", "", "", "", "", "", ""  # H-M: Options, Answer, Explanation
                ])
            elif "info" in q:
                rows.append([
                    "INFO",                     # C: Type
                    "",                         # D: Number
                    q.get("file_name", ""),     # E: File Name
                    "",                         # F: Instructions
                    q.get("info", ""),          # G: Text
                    "", "", "", "", "", "", ""  # H-M: Options, Answer, Explanation
                ])
            else:
                rows.append([
                    q.get("type", ""),          # C: Type
                    q.get("number", ""),        # D: Number
                    q.get("file_name", ""),     # E: File Name
                    q.get("instructions", ""),  # F: Instructions
                    q.get("text", ""),          # G: Text
                    q.get("option_a", ""),      # H: Option A
                    q.get("option_b", ""),      # I: Option B
                    q.get("option_c", ""),      # J: Option C
                    q.get("option_d", ""),      # K: Option D
                    q.get("answer", ""),        # L: Answer
                    q.get("explanation", "")    # M: Explanation
                ])
        
        try:
            if rows:
                range_name = f"{self.sheet_tab}!C{self.next_output_row}"
                body = {"values": rows}
                
                self.service.spreadsheets().values().update(
                    spreadsheetId=self.sheet_id,
                    range=range_name,
                    valueInputOption="RAW",
                    body=body
                ).execute()
                
                print(f"Updated sheet with {len(rows)} rows (starting at row {self.next_output_row})")
                self.next_output_row += len(rows)
        except Exception as e:
            print(f"Error updating sheet: {e}")
    
    def process_all_pdfs(self, batch_size: int = 1, resume: bool = True):
        """Main processing function"""
        print(f"\n{'='*80}")
        print("STARTING PDF QUESTION EXTRACTION")
        print(f"{'='*80}")
        
        # Read sheet data
        sheet_data = self.read_sheet_data()
        if len(sheet_data) < 2:
            print("No data found in sheet (need at least header + 1 row)")
            return
        
        # Load processed files
        processed_files = self.load_processed_files()
        
        # Clear output if not resuming or no progress file
        if not resume or not processed_files:
            self.clear_output_columns()
            self.next_output_row = 2
            print("Starting fresh processing")
        else:
            print(f"Resuming from previous progress ({len(processed_files)} files already processed)")
        
        # Prepare files to process
        files_to_process = []
        skipped_count = 0
        
        for i, row in enumerate(sheet_data[1:], start=2):  # Skip header
            if len(row) < 2:
                print(f"Row {i}: Insufficient data")
                continue
            
            file_name = row[0].strip()
            pdf_url = row[1].strip()
            
            if not pdf_url.startswith("http"):
                print(f"Row {i}: Invalid URL for {file_name}")
                continue
            
            # Check if file is already processed
            if resume and file_name in processed_files:
                skipped_count += 1
                continue
            
            files_to_process.append((file_name, pdf_url))
        
        print(f"\nPROCESSING SUMMARY:")
        print(f"   Total rows in sheet: {len(sheet_data) - 1}")
        print(f"   Already processed: {skipped_count}")
        print(f"   To process: {len(files_to_process)}")
        print(f"   Batch size: {batch_size}")
        
        if not files_to_process:
            print("\nAll PDFs already processed!")
            return
        
        # Process in batches
        all_questions = []
        pending_questions = []
        
        for batch_start in range(0, len(files_to_process), batch_size):
            batch_end = min(batch_start + batch_size, len(files_to_process))
            current_batch = files_to_process[batch_start:batch_end]
            
            batch_num = (batch_start // batch_size) + 1
            total_batches = (len(files_to_process) - 1) // batch_size + 1
            
            print(f"\n{'='*60}")
            print(f"BATCH {batch_num}/{total_batches}")
            print(f"{'='*60}")
            print(f"Files in this batch:")
            for name, _ in current_batch:
                print(f"  • {name}")
            
            # Process batch
            batch_results = self.extractor.process_pdf_batch(current_batch)
            
            # Mark files as processed
            for name, _ in current_batch:
                processed_files.add(name)
            
            # Save progress
            self.save_processed_files(processed_files)
            
            # Filter valid questions
            valid_questions = [q for q in batch_results if isinstance(q, dict) and q.get("text")]
            all_questions.extend(valid_questions)
            pending_questions.extend(valid_questions)
            
            # Update sheet if we have enough questions
            if len(pending_questions) >= UPDATE_BATCH_SIZE:
                print(f"\nUpdating sheet with {len(pending_questions)} questions...")
                self.update_sheet_with_questions(pending_questions)
                pending_questions = []
            
            # Rate limiting between batches
            if batch_end < len(files_to_process):
                print(f"\nWaiting 5 seconds before next batch...")
                time.sleep(5)
        
        # Final update for remaining questions
        if pending_questions:
            print(f"\nFinal update with {len(pending_questions)} remaining questions...")
            self.update_sheet_with_questions(pending_questions)
        
        # Print statistics
        self.extractor.print_statistics()
        
        # Final summary
        print(f"\n{'='*80}")
        print("PROCESSING COMPLETE!")
        print(f"{'='*80}")
        print(f"Total files processed: {len(files_to_process)}")
        print(f"Total questions extracted: {len(all_questions)}")
        print(f"Questions saved to: {self.sheet_tab}!C{2}:M{self.next_output_row-1}")
        print(f"Columns: Type, Number, File, Instructions, Text, A, B, C, D, Answer, Explanation")
        print(f"Progress saved to: {PROGRESS_FILE}")
        print(f"{'='*80}")

# ==============================
#  INSTALLATION CHECK
# ==============================
def check_installations():
    """Check if required packages are installed"""
    required_packages = [
        ('requests', 'requests'),
        ('PyPDF2', 'PyPDF2'),
        ('pdfplumber', 'pdfplumber'),
        ('google-auth', 'google.oauth2'),
    ]
    
    missing_packages = []
    
    for package_name, import_name in required_packages:
        try:
            __import__(import_name)
            print(f"{package_name}")
        except ImportError:
            print(f"{package_name} - MISSING")
            missing_packages.append(package_name)
    
    return missing_packages

# ==============================
#  MAIN FUNCTION
# ==============================
def main():
    print("\n" + "="*80)
    print("   DEEPSEEK PDF QUESTION EXTRACTION SYSTEM")
    print("="*80)
    
    # Check API key
    if DEEPSEEK_API_KEY == "your_deepseek_api_key_here":
        print("ERROR: Please replace 'your_deepseek_api_key_here' with your actual DeepSeek API key")
        print("Get your key from: https://platform.deepseek.com/api_keys")
        print("\nOnce you have your API key, edit the script and replace the placeholder.")
        return
    
    # Check installations
    print("\nChecking required packages...")
    missing = check_installations()
    
    if missing:
        print(f"\nMissing packages detected. Install with:")
        print(f"pip install {' '.join(missing)}")
        return
    
    print("\nAll required packages are installed")
    
    # Check credentials file
    if not os.path.exists(CREDENTIALS_FILE):
        print(f"\nCredentials file not found: {CREDENTIALS_FILE}")
        print("Make sure you have downloaded your Google Service Account JSON file.")
        return
    
    # Configuration
    BATCH_SIZE = 1  # Process one PDF at a time for reliability
    RESUME_MODE = True
    
    print(f"\nCONFIGURATION:")
    print(f"   DeepSeek Model: {EXTRACTION_MODEL}")
    print(f"   Google Sheet: {GOOGLE_SHEET_ID}")
    print(f"   Sheet Tab: {SHEET_TAB}")
    print(f"   Batch Size: {BATCH_SIZE}")
    print(f"   Resume Mode: {'Yes' if RESUME_MODE else 'No'}")
    
    # Start processing
    try:
        processor = GoogleSheetsPDFProcessor(
            credentials_file=CREDENTIALS_FILE,
            sheet_id=GOOGLE_SHEET_ID,
            api_key=DEEPSEEK_API_KEY,
            sheet_tab=SHEET_TAB
        )
        
        processor.process_all_pdfs(batch_size=BATCH_SIZE, resume=RESUME_MODE)
        
    except KeyboardInterrupt:
        print("\n\nProcessing interrupted by user")
    except Exception as e:
        print(f"\nFatal error: {e}")
        import traceback
        traceback.print_exc()

# ==============================
#  STANDALONE TEST FUNCTION
# ==============================
def test_single_pdf():
    """Test function for single PDF without Google Sheets"""
    print("\n" + "="*80)
    print("   TEST SINGLE PDF EXTRACTION")
    print("="*80)
    
    if DEEPSEEK_API_KEY == "your_deepseek_api_key_here":
        print("Please update API key first")
        return
    
    print("\nEnter PDF URL (or press Enter for test URL):")
    pdf_url = input().strip()
    
    if not pdf_url:
        pdf_url = "https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf"
        print(f"Using test URL: {pdf_url}")
    
    print(f"\nProcessing: {pdf_url}")
    
    extractor = DeepSeekPDFQuestionExtractor(DEEPSEEK_API_KEY)
    
    # Download PDF
    pdf_bytes = extractor.download_pdf(pdf_url)
    if not pdf_bytes:
        print("Failed to download PDF")
        return
    
    # Extract questions
    questions = extractor.extract_questions_from_pdf(pdf_bytes, "test.pdf")
    
    # Save results
    output_file = "test_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(questions, f, indent=2, ensure_ascii=False)
    
    print(f"\nResults saved to: {output_file}")
    print(f"\nSummary: {len([q for q in questions if isinstance(q, dict) and q.get('text')])} questions extracted")
    
    # Print sample with answer and explanation
    for i, q in enumerate(questions[:3]):
        if isinstance(q, dict) and q.get('text'):
            print(f"\nSample Question {i+1}:")
            print(f"  Number: {q.get('number', 'N/A')}")
            print(f"  Question: {q.get('text', '')[:100]}...")
            print(f"  Answer: {q.get('answer', 'No answer extracted')}")
            print(f"  Explanation: {q.get('explanation', 'No explanation extracted')[:100]}...")

# ==============================
#  ENTRY POINT
# ==============================
if __name__ == "__main__":
    print("Choose mode:")
    print("1. Full Google Sheets processing (main)")
    print("2. Test single PDF extraction")
    print("3. Check installations")
    print("4. Exit")
    
    choice = input("\nEnter choice (1/2/3/4): ").strip()
    
    if choice == "2":
        test_single_pdf()
    elif choice == "3":
        check_installations()
    elif choice == "4":
        print("Exiting...")
    else:
        main()
