import csv, os, time, requests, pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# --------------------------------------------
INPUT_FILE = "urls-2.csv"
URL_COLUMN = "URL"
LEVEL1_CLASS = "sample-links mb-3 ul1"
LEVEL2_CLASSES = ["colored-links mb-3", "box-links mb-3 row row-cols-2 row-cols-sm-3 row-cols-lg-3 row-cols-xl-3 ul1"]
OUTPUT_CSV = "hierarchy_progress3.csv"
OUTPUT_TXT = "hierarchy_progress3.txt"
SAVE_INTERVAL = 5
# --------------------------------------------


# ========== SELENIUM SETUP ==========
def start_selenium():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=opts)
    driver.set_page_load_timeout(25)
    return driver

def stop_selenium(d): 
    try: d.quit()
    except: pass


# ========== FETCHING ==========
def get_html(url, driver=None):
    """Try requests first, fallback to Selenium if needed"""
    try:
        r = requests.get(url, timeout=8)
        if r.status_code == 200 and "<ul" in r.text:
            return r.text
    except Exception:
        pass
    if not driver:
        return None
    try:
        driver.get(url)
        time.sleep(3)
        return driver.page_source
    except Exception:
        return None


# ========== PARSING ==========
def extract_level1_links(html, base_url):
    """Extract Parent ‚Üí Child links"""
    soup = BeautifulSoup(html, "html.parser")
    uls = soup.find_all("ul", class_=LEVEL1_CLASS)
    data, current_parent = [], None
    for ul in uls:
        for li in ul.find_all("li"):
            a = li.find("a")
            if not a: 
                continue
            text = a.get_text(strip=True)
            href = a.get("href", "").strip()
            full_link = urljoin(base_url, href)
            if "javascript:void(0)" in href:
                current_parent = text
            else:
                data.append((current_parent, text, full_link))
    return data


def extract_level2_links(html, base_url):
    """Extract sublinks from either colored-links or box-links structure"""
    soup = BeautifulSoup(html, "html.parser")
    results = []
    # --- Colored-links ---
    for cls in LEVEL2_CLASSES:
        uls = soup.find_all("ul", class_=cls)
        for ul in uls:
            for li in ul.find_all("li"):
                a = li.find("a")
                if not a:
                    continue
                text = a.get_text(strip=True)
                href = a.get("href", "").strip()
                full_link = urljoin(base_url, href)
                results.append((text, full_link))
    return results


# ========== FILE MANAGEMENT ==========
def load_existing_progress():
    """Read existing CSV to know which URLs are complete/failed"""
    if not os.path.exists(OUTPUT_CSV):
        return pd.DataFrame(), set(), set()
    df = pd.read_csv(OUTPUT_CSV, encoding="utf-8-sig")
    done = set(df.loc[df["Status"].str.contains("Success", na=False), "Parent URL"].dropna())
    failed = set(df.loc[~df["Status"].str.contains("Success", na=False), "Parent URL"].dropna())
    return df, done, failed


def append_to_files(rows, headers):
    """Save new batch to CSV + TXT"""
    exists = os.path.exists(OUTPUT_CSV)
    with open(OUTPUT_CSV, "a", encoding="utf-8-sig", newline="") as f:
        w = csv.writer(f)
        if not exists:
            w.writerow(headers)
        w.writerows(rows)

    with open(OUTPUT_TXT, "a", encoding="utf-8-sig") as f:
        for r in rows:
            bc, st = r[-2], r[-1]
            if "Success" in st:
                f.write(f"{bc} -> {r[6] or r[5]}\n")
            else:
                f.write(f"[{st}] {bc}\n")


# ========== MAIN CRAWLER ==========
def crawl_all():
    df_in = pd.read_csv(INPUT_FILE, encoding="utf-8-sig")
    if URL_COLUMN not in df_in.columns:
        raise ValueError(f"Missing column '{URL_COLUMN}' in input file")

    df_old, done, failed = load_existing_progress()
    driver = start_selenium()
    results = []
    total_processed = 0

    print(f"üîÅ Resuming: {len(done)} done, {len(failed)} failed to retry")

    for i, row in df_in.iterrows():
        url = str(row[URL_COLUMN]).strip()
        if not url or not url.startswith("http"): 
            continue
        if url in done: 
            continue  # skip already successful

        meta = [str(x) if str(x) != "nan" else "" for x in row]
        print(f"\nüîç ({i+1}/{len(df_in)}) {url}")

        try:
            html = get_html(url, driver)
            if not html:
                raise Exception("Empty page")
            lvl1 = extract_level1_links(html, url)
            if not lvl1:
                # Even if no Level-1, try Level-2 directly
                lvl2 = extract_level2_links(html, url)
                if lvl2:
                    for stext, slink in lvl2:
                        bc = f"{stext}"
                        results.append([*meta, "", "", stext, url, "", slink, bc, "Success"])
                else:
                    results.append([*meta, "", "", "", url, "", "", "", "Empty: no box-links or colored-links found"])
            else:
                for parent, child, curl in lvl1:
                    bc = f"{parent} > {child}" if parent else child
                    results.append([*meta, parent, child, "", url, curl, "", bc, "Success"])
                    sub_html = get_html(curl, driver)
                    if not sub_html:
                        results.append([*meta, parent, child, "", url, curl, "", bc, "Failed: No sub-HTML"])
                        continue
                    lvl2 = extract_level2_links(sub_html, curl)
                    if not lvl2:
                        results.append([*meta, parent, child, "", url, curl, "", bc, "Empty: no Level2 links"])
                    for stext, slink in lvl2:
                        bc2 = f"{parent} > {child} > {stext}" if parent else f"{child} > {stext}"
                        results.append([*meta, parent, child, stext, url, curl, slink, bc2, "Success"])

        except Exception as e:
            results.append([*meta, "", "", "", url, "", "", "", f"Failed: {e}"])

        total_processed += 1
        append_to_files(
            results,
            list(df_in.columns) + [
                "Parent", "Child", "SubChild",
                "Parent URL", "Child URL", "SubChild URL",
                "Breadcrumb", "Status"
            ]
        )
        results = []
        print(f"üíæ Saved progress after {total_processed} URLs")

    stop_selenium(driver)
    print(f"\n‚úÖ Completed! Total processed: {total_processed}")
    print(f"üìÅ Files:\n - {OUTPUT_CSV}\n - {OUTPUT_TXT}")


if __name__ == "__main__":
    crawl_all()
