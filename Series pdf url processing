import os
import time
import requests
import re
import csv
from urllib.parse import urlparse, unquote
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import random
import hashlib
import glob

# Set the save directory to E:\pdf
SAVE_DIR = r"C:\Users\menha\Downloads\urls"
TEXT_FILES_DIR = r"C:\Users\menha\Downloads\urls"  # Directory containing your text files
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(TEXT_FILES_DIR, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(SAVE_DIR, 'metadata_scraper.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FastMetadataSelfStudysScraper:
    def __init__(self, max_workers=5):  # Reduced workers to save resources
        self.base_url = "https://www.selfstudys.com"
        self.metadata_file = os.path.join(SAVE_DIR, 'metadata_collection.csv')
        self.processed_urls = self.load_processed_urls()
        self.interrupted = False
        self.session = self.create_session()
        self.max_workers = max_workers
        self.current_file_index = 0
        self.text_files = self.get_text_files()
        
        # Setup signal handler for graceful interruption
        signal.signal(signal.SIGINT, self.signal_handler)
    
    def get_text_files(self):
        """Get all text files in the specified directory"""
        text_files = glob.glob(os.path.join(TEXT_FILES_DIR, "*.txt"))
        text_files.sort()  # Sort alphabetically
        logger.info(f"Found {len(text_files)} text files to process")
        return text_files
    
    def create_session(self):
        """Create a requests session with retry logic"""
        session = requests.Session()
        retry_strategy = Retry(
            total=2,
            backoff_factor=0.3,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=20, pool_maxsize=20)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session
    
    def signal_handler(self, sig, frame):
        """Handle Ctrl+C interruption gracefully"""
        logger.info("\n\nâš ï¸  Interruption received. Saving progress and shutting down gracefully...")
        self.interrupted = True
        self.save_progress()
        sys.exit(0)
    
    def load_processed_urls(self) -> set:
        """Load already processed URLs from metadata file"""
        processed_urls = set()
        
        if os.path.exists(self.metadata_file):
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    next(reader, None)  # Skip header
                    for row in reader:
                        if len(row) > 0:
                            processed_urls.add(row[0])  # URL is in the first column
            except Exception as e:
                logger.error(f"Error reading metadata file: {e}")
        
        logger.info(f"Loaded {len(processed_urls)} processed URLs from metadata")
        return processed_urls
    
    def read_urls_from_file(self, filename: str) -> list:
        """Read URLs from external text file efficiently"""
        urls = []
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                for line in f:
                    url = line.strip()
                    if url and not url.startswith('#'):
                        urls.append(url)
            logger.info(f"Loaded {len(urls)} URLs from {os.path.basename(filename)}")
        except FileNotFoundError:
            logger.error(f"Error: File {filename} not found.")
        
        return urls
    
    def load_checkpoint(self) -> dict:
        """Load checkpoint data to resume from where we left off"""
        checkpoint_file = os.path.join(SAVE_DIR, 'metadata_checkpoint.txt')
        checkpoint_data = {
            'last_processed_index': 0,
            'current_file_index': 0
        }
        
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith('Last_Processed_Index:'):
                            checkpoint_data['last_processed_index'] = int(line.split(':')[1].strip())
                        elif line.startswith('Current_File_Index:'):
                            checkpoint_data['current_file_index'] = int(line.split(':')[1].strip())
                logger.info(f"Resuming from file index {checkpoint_data['current_file_index']}, URL index {checkpoint_data['last_processed_index']}")
            except:
                logger.info("Could not read checkpoint file, starting from beginning")
        
        return checkpoint_data
    
    def save_checkpoint(self, url_index: int, file_index: int, total_urls: int):
        """Save current progress to checkpoint file"""
        checkpoint_file = os.path.join(SAVE_DIR, 'metadata_checkpoint.txt')
        try:
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                f.write(f"Last_Run: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Last_Processed_Index: {url_index}\n")
                f.write(f"Current_File_Index: {file_index}\n")
                f.write(f"Total_URLs: {total_urls}\n")
                f.write(f"Processed_URLs: {len(self.processed_urls)}\n")
                elapsed = time.time() - self.start_time
                urls_per_hour = (url_index / elapsed) * 3600 if elapsed > 0 else 0
                f.write(f"URLs_Per_Hour: {urls_per_hour:.2f}\n")
                f.write(f"Current_File: {os.path.basename(self.text_files[file_index]) if file_index < len(self.text_files) else 'COMPLETED'}\n")
        except Exception as e:
            logger.error(f"Error saving checkpoint: {e}")
    
    def is_sitepdfs_url(self, url: str) -> bool:
        """Check if the URL is a sitepdfs URL"""
        return '/sitepdfs/' in url
    
    def is_direct_pdf_url(self, url: str) -> bool:
        """Check if the URL points directly to a PDF file"""
        return url.lower().endswith('.pdf') or '.pdf?' in url.lower() or self.is_sitepdfs_url(url)
    
    def clean_filename(self, filename: str) -> str:
        """Clean filename to remove problematic characters"""
        # Remove problematic characters but allow dots, hyphens, and spaces
        clean_name = re.sub(r'[<>:"/\\|?*]', '_', filename)
        clean_name = clean_name.strip()
        
        # Replace multiple spaces/underscores with single underscore
        clean_name = re.sub(r'[\s_]+', '_', clean_name)
        
        # Limit length
        return clean_name[:150]
    
    def extract_pdf_url_from_html(self, html_content: str, base_url: str) -> str:
        """Extract PDF URL from HTML content using regex patterns"""
        patterns = [
            r'<embed[^>]+src="([^"]+\.pdf[^"]*)"',
            r'<iframe[^>]+src="([^"]+\.pdf[^"]*)"',
            r'<object[^>]+data="([^"]+\.pdf[^"]*)"',
            r'data-pdf-url="([^"]+)"',
            r'data-url="([^"]+\.pdf[^"]*)"',
            r'data-src="([^"]+\.pdf[^"]*)"',
            r'href="([^"]+\.pdf[^"]*)"',
            r'src="([^"]+\.pdf[^"]*)"',
            r'https?://[^"\']+\.pdf',
            r'/sitepdfs/[^"\']+',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if '.pdf' in match.lower() or '/sitepdfs/' in match:
                    # Ensure the URL is absolute
                    if match.startswith('//'):
                        return 'https:' + match
                    elif match.startswith('/'):
                        return self.base_url + match
                    elif match.startswith('http'):
                        return match
                    else:
                        return self.base_url + '/' + match
        
        return None
    
    def extract_title_from_html(self, html_content: str) -> str:
        """Extract document title from HTML content"""
        # Try to extract from title tag
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE)
        if title_match:
            title = title_match.group(1).strip()
            if title and len(title) > 10 and 'download' not in title.lower():
                return title
        
        # Try to extract from h1-h3 tags
        for tag in ['h1', 'h2', 'h3']:
            pattern = fr'<{tag}[^>]*>(.*?)</{tag}>'
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                title = re.sub(r'<[^>]+>', '', match).strip()  # Remove HTML tags
                if title and len(title) > 10 and 'download' not in title.lower():
                    return title
        
        # Try to extract from meta tags
        meta_patterns = [
            r'<meta[^>]+property="og:title"[^>]+content="([^"]+)"',
            r'<meta[^>]+name="title"[^>]+content="([^"]+)"'
        ]
        
        for pattern in meta_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if match and len(match) > 10 and 'download' not in match.lower():
                    return match
        
        return None
    
    def generate_filename_from_title(self, title: str, url: str) -> str:
        """Generate filename from document title with improved character handling"""
        if title:
            # Clean the title for filename use - allow dots, hyphens, and spaces
            clean_title = re.sub(r'[<>:"/\\|?*]', '_', title)
            clean_title = clean_title.strip()
            
            # Replace multiple spaces with single space
            clean_title = re.sub(r'\s+', ' ', clean_title)
            
            # Limit length and ensure it ends with .pdf
            filename = clean_title[:100] + '.pdf'
            return filename
        else:
            # Fallback to URL-based filename if title not found
            return self.generate_filename_from_url(url)
    
    def generate_filename_from_url(self, url: str) -> str:
        """Generate filename from URL as fallback with improved character handling"""
        # For direct PDF URLs, extract filename from URL
        if self.is_direct_pdf_url(url):
            parsed_url = urlparse(url)
            path_segments = [p for p in parsed_url.path.split('/') if p]
            
            if path_segments:
                filename = unquote(path_segments[-1])
                if not filename.lower().endswith('.pdf'):
                    filename += '.pdf'
                return self.clean_filename(filename)
        
        # For regular URLs, use the previous logic
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p and p != 'advance-pdf-viewer']
        
        # Use the last 2-3 parts to create the filename
        if len(path_parts) >= 2:
            filename = "_".join(path_parts[-2:])
        else:
            filename = path_parts[-1] if path_parts else "document"
        
        # Clean the filename - allow dots, hyphens, and spaces
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        filename = filename.strip('_')
        
        # Replace multiple underscores with single underscore
        filename = re.sub(r'_+', '_', filename)
        
        # Ensure the filename ends with .pdf
        if not filename.endswith('.pdf'):
            filename += '.pdf'
        
        # Limit filename length
        return self.clean_filename(filename)
    
    def collect_metadata_fast(self, viewer_url: str) -> bool:
        """Collect metadata for a URL using fast HTTP requests instead of Selenium"""
        # Check if this is a direct PDF URL or sitepdfs URL
        if self.is_direct_pdf_url(viewer_url):
            logger.info(f"Direct PDF URL detected: {viewer_url}")
            filename = self.generate_filename_from_url(viewer_url)
            self.save_metadata(viewer_url, viewer_url, filename, "direct_pdf")
            self.processed_urls.add(viewer_url)
            return True
        
        try:
            # Use a HEAD request first to check if it redirects to a PDF
            head_response = self.session.head(viewer_url, timeout=10, allow_redirects=True)
            final_url = head_response.url
            
            # Check if the final URL is a direct PDF
            if self.is_direct_pdf_url(final_url) and final_url != viewer_url:
                logger.info(f"URL redirects to direct PDF: {final_url}")
                filename = self.generate_filename_from_url(final_url)
                self.save_metadata(viewer_url, final_url, filename, "redirected_pdf")
                self.processed_urls.add(viewer_url)
                return True
            
            # If not a direct PDF, fetch the HTML content
            response = self.session.get(viewer_url, timeout=15)
            
            if response.status_code != 200:
                logger.warning(f"Failed to fetch {viewer_url}: HTTP {response.status_code}")
                filename = self.generate_filename_from_url(viewer_url)
                self.save_metadata(viewer_url, "error", filename, f"http_error_{response.status_code}")
                self.processed_urls.add(viewer_url)
                return False
            
            html_content = response.text
            
            # Extract PDF URL from HTML
            pdf_url = self.extract_pdf_url_from_html(html_content, viewer_url)
            
            # Extract title from HTML
            title = self.extract_title_from_html(html_content)
            
            if pdf_url:
                # Generate filename from document title or URL
                filename = self.generate_filename_from_title(title, viewer_url)
                
                self.save_metadata(viewer_url, pdf_url, filename, "extracted_pdf")
                self.processed_urls.add(viewer_url)
                return True
            else:
                logger.warning(f"Could not find PDF URL for: {viewer_url}")
                # Check for download buttons/links in HTML
                download_patterns = [
                    r'<a[^>]+download[^>]*>',
                    r'<a[^>]+href[^>]*\.pdf[^>]*>',
                    r'<button[^>]+download[^>]*>',
                ]
                
                for pattern in download_patterns:
                    if re.search(pattern, html_content, re.IGNORECASE):
                        logger.info(f"Found download button/link on: {viewer_url}")
                        filename = self.generate_filename_from_url(viewer_url)
                        self.save_metadata(viewer_url, "unknown", filename, "has_download_buttons")
                        self.processed_urls.add(viewer_url)
                        return True
            
            # If we couldn't find any PDF URL, still record the metadata
            filename = self.generate_filename_from_url(viewer_url)
            self.save_metadata(viewer_url, "unknown", filename, "no_pdf_found")
            self.processed_urls.add(viewer_url)
            return True
            
        except Exception as e:
            logger.error(f"Metadata collection failed for {viewer_url}: {e}")
            
            # Even if failed, record the URL to avoid reprocessing
            filename = self.generate_filename_from_url(viewer_url)
            self.save_metadata(viewer_url, "error", filename, f"error: {str(e)}")
            self.processed_urls.add(viewer_url)
            return False
    
    def save_metadata(self, viewer_url: str, pdf_url: str, filename: str, status: str):
        """Save download metadata to CSV efficiently"""
        file_exists = os.path.isfile(self.metadata_file)
        
        try:
            with open(self.metadata_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                if not file_exists:
                    writer.writerow(['Viewer_URL', 'PDF_URL', 'Filename', 'Status', 'Timestamp'])
                
                writer.writerow([
                    viewer_url,
                    pdf_url,
                    filename,
                    status,
                    time.strftime('%Y-%m-%d %H:%M:%S')
                ])
        except Exception as e:
            logger.error(f"Error saving metadata: {e}")
    
    def process_url_batch(self, urls_batch: list, batch_num: int, total_batches: int):
        """Process a batch of URLs"""
        successful = 0
        failed = 0
        
        for i, url in enumerate(urls_batch):
            if self.interrupted:
                break
                
            if i % 100 == 0:  # Reduced logging frequency
                logger.info(f"Batch {batch_num}/{total_batches}: Processed {i}/{len(urls_batch)} URLs")
            
            if self.collect_metadata_fast(url):
                successful += 1
            else:
                failed += 1
            
            # Add a very small random delay to avoid detection
            time.sleep(random.uniform(0.01, 0.05))  # Reduced delay
        
        return successful, failed
    
    def save_progress(self):
        """Save progress to a checkpoint file"""
        checkpoint_file = os.path.join(SAVE_DIR, 'metadata_checkpoint.txt')
        try:
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                f.write(f"Last_Run: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Processed_URLs: {len(self.processed_urls)}\n")
        except Exception as e:
            logger.error(f"Error saving progress: {e}")
    
    def run_metadata_collection(self, batch_size: int = 1000):  # Increased batch size
        """Run the metadata collection process with parallel processing for all text files"""
        self.start_time = time.time()
        logger.info("ðŸš€ Starting FAST SelfStudys Metadata Collection (No Downloads)...")
        logger.info("=" * 60)
        logger.info(f"Metadata will be saved to: {self.metadata_file}")
        logger.info(f"Text files directory: {TEXT_FILES_DIR}")
        logger.info(f"Found {len(self.text_files)} text files to process")
        logger.info(f"Max workers: {self.max_workers}")
        logger.info("=" * 60)
        
        if not self.text_files:
            logger.info("No text files found. Exiting.")
            return
        
        # Load checkpoint to resume from where we left off
        checkpoint_data = self.load_checkpoint()
        current_file_index = checkpoint_data['current_file_index']
        start_index = checkpoint_data['last_processed_index']
        
        total_successful_all_files = 0
        total_failed_all_files = 0
        
        # Process each text file sequentially
        for file_index in range(current_file_index, len(self.text_files)):
            if self.interrupted:
                break
                
            current_file = self.text_files[file_index]
            logger.info(f"Processing file {file_index + 1}/{len(self.text_files)}: {os.path.basename(current_file)}")
            
            # Read URLs from current file
            all_urls = self.read_urls_from_file(current_file)
            
            if not all_urls:
                logger.info(f"No URLs found in {os.path.basename(current_file)}. Moving to next file.")
                continue
            
            # Filter out already processed URLs
            new_urls = [url for url in all_urls if url not in self.processed_urls]
            
            if not new_urls:
                logger.info(f"All URLs in {os.path.basename(current_file)} have already been processed. Moving to next file.")
                start_index = 0  # Reset for next file
                continue
            
            logger.info(f"URLs to process in this file: {len(new_urls)}")
            
            # Process URLs in smaller batches to manage memory
            batch_size = min(batch_size, 1000)  # Cap at 1000 URLs per batch
            batches = [new_urls[i:i+batch_size] for i in range(0, len(new_urls), batch_size)]
            total_batches = len(batches)
            
            total_successful = 0
            total_failed = 0
            
            # Process batches sequentially (not in parallel) to reduce resource usage
            for batch_num, batch in enumerate(batches):
                if self.interrupted:
                    break
                    
                logger.info(f"Processing batch {batch_num+1}/{total_batches} ({len(batch)} URLs)")
                successful, failed = self.process_url_batch(batch, batch_num+1, total_batches)
                total_successful += successful
                total_failed += failed
                
                # Save progress after each batch
                current_url_index = start_index + total_successful + total_failed
                self.save_checkpoint(current_url_index, file_index, len(all_urls))
            
            # Update totals for all files
            total_successful_all_files += total_successful
            total_failed_all_files += total_failed
            
            # Print summary for this file
            logger.info(f"Completed file: {os.path.basename(current_file)}")
            logger.info(f"Successful in this file: {total_successful}")
            logger.info(f"Failed in this file: {total_failed}")
            
            # Reset start index for next file
            start_index = 0
        
        # Print final summary
        logger.info("=" * 60)
        logger.info("ðŸ“Š METADATA COLLECTION SUMMARY")
        logger.info(f"Total files processed: {len(self.text_files)}")
        logger.info(f"Total URLs processed: {total_successful_all_files + total_failed_all_files}")
        logger.info(f"Successful metadata collection: {total_successful_all_files}")
        logger.info(f"Failed metadata collection: {total_failed_all_files}")
        if (total_successful_all_files + total_failed_all_files) > 0:
            success_rate = (total_successful_all_files/(total_successful_all_files + total_failed_all_files))*100
            logger.info(f"Success rate: {success_rate:.1f}%")
        
        # Calculate and display performance metrics
        elapsed = time.time() - self.start_time
        hours = elapsed / 3600
        if hours > 0:
            urls_per_hour = total_successful_all_files / hours
            logger.info(f"Processing speed: {urls_per_hour:.2f} URLs/hour")
        
        logger.info(f"Metadata saved in: {self.metadata_file}")

# Run the metadata collector
if __name__ == "__main__":
    # Configuration - adjust based on your system capabilities
    MAX_WORKERS = 2  # Reduced worker count to save resources
    BATCH_SIZE = 1000  # URLs per batch
    
    scraper = FastMetadataSelfStudysScraper(max_workers=MAX_WORKERS)
    scraper.run_metadata_collection(batch_size=BATCH_SIZE)
