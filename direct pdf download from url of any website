import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import urllib3
import time

# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_URL = "https://jam2025.iitd.ac.in/masterqp.php"
SAVE_DIR = "downloads_papers"
os.makedirs(SAVE_DIR, exist_ok=True)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# Create a single Session for connection pooling (faster & more stable)
session = requests.Session()
session.headers.update(HEADERS)

print("Fetching page...")
response = session.get(BASE_URL, verify=False, timeout=30)
response.raise_for_status()

soup = BeautifulSoup(response.text, "html.parser")

# Collect all PDF links
pdf_links = []
for link in soup.find_all("a", href=True):
    href = link["href"]
    if href.lower().endswith(".pdf"):
        full_url = urljoin(BASE_URL, href)
        pdf_links.append(full_url)

print(f"Found {len(pdf_links)} PDF files.")

# Retry configuration
MAX_RETRIES = 5
RETRY_DELAY = 10  # seconds between retries

def download_pdf(url):
    pdf_name = os.path.basename(url.split("?")[0])
    save_path = os.path.join(SAVE_DIR, pdf_name)

    # Skip if file already downloaded
    if os.path.exists(save_path) and os.path.getsize(save_path) > 0:
        return f"✅ Skipped (exists): {pdf_name}"

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            r = session.get(url, stream=True, verify=False, timeout=60)
            r.raise_for_status()
            with open(save_path, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            return f"✅ Downloaded: {pdf_name}"
        except Exception as e:
            if attempt < MAX_RETRIES:
                print(f"⚠️  Retry {attempt}/{MAX_RETRIES} for {pdf_name} in {RETRY_DELAY}s due to {e}")
                time.sleep(RETRY_DELAY)
            else:
                return f"❌ Failed after {MAX_RETRIES} retries: {pdf_name} ({e})"

MAX_THREADS = 10  # Increase to 20 if you have fast internet

print(f"Starting downloads with {MAX_THREADS} threads...")
with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
    futures = [executor.submit(download_pdf, url) for url in pdf_links]
    for i, future in enumerate(as_completed(futures), 1):
        print(f"[{i}/{len(pdf_links)}] {future.result()}")

print("\n🎯 All downloads complete!")
