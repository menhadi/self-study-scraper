import os
import time
import requests
import re
import csv
from urllib.parse import urlparse, unquote
from concurrent.futures import ThreadPoolExecutor, as_completed
import signal
import sys
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import random
import pandas as pd
from typing import List, Tuple

# Set the save directory to D:\Self Study\Pdf for maximum speed
SAVE_DIR = r"D:\Self Study\Pdf"
os.makedirs(SAVE_DIR, exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(SAVE_DIR, 'scraper.log')),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class UltraFastPDFDownloader:
    def __init__(self, max_workers=100):
        self.downloaded_urls = self.load_downloaded_urls()
        self.interrupted = False
        self.start_time = time.time()
        self.session = self.create_session()
        self.max_workers = max_workers
        
        # Setup signal handler
        signal.signal(signal.SIGINT, self.signal_handler)
    
    def create_session(self):
        """Create ultra-fast requests session"""
        session = requests.Session()
        retry_strategy = Retry(
            total=1,  # Only 1 retry for speed
            backoff_factor=0.1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=500, pool_maxsize=500)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session
    
    def signal_handler(self, sig, frame):
        """Handle interruption"""
        logger.info("\nInterruption received. Shutting down...")
        self.interrupted = True
        sys.exit(0)
    
    def load_downloaded_urls(self):
        """Load downloaded URLs quickly"""
        downloaded_urls = set()
        csv_file = os.path.join(SAVE_DIR, 'downloads_metadata.csv')
        
        if os.path.exists(csv_file):
            try:
                with open(csv_file, 'r', encoding='utf-8') as f:
                    reader = csv.reader(f)
                    next(reader, None)
                    for row in reader:
                        if len(row) > 1:
                            downloaded_urls.add(row[1])  # Viewer_URL
            except Exception as e:
                logger.error(f"Error reading metadata: {e}")
        
        return downloaded_urls
    
    def read_urls_from_excel(self, excel_file: str) -> List[Tuple[str, str, str]]:
        """Read URLs and filenames from Excel file"""
        urls_data = []
        try:
            # Read Excel file
            df = pd.read_excel(excel_file)
            
            # Check if required columns exist
            required_columns = ['Viewer_URL', 'PDF_URL', 'Filename']
            for col in required_columns:
                if col not in df.columns:
                    logger.error(f"Column '{col}' not found in Excel file")
                    return []
            
            # Process each row
            for index, row in df.iterrows():
                viewer_url = str(row['Viewer_URL']).strip()
                pdf_url = str(row['PDF_URL']).strip()
                filename = str(row['Filename']).strip()
                
                if viewer_url and viewer_url != 'nan' and pdf_url and pdf_url != 'nan':
                    # Ensure filename has .pdf extension
                    if filename and filename != 'nan':
                        if not filename.lower().endswith('.pdf'):
                            filename += '.pdf'
                        # Clean filename
                        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
                        filename = filename.strip()[:150]
                    else:
                        # Generate filename from URL if not provided
                        filename = self.generate_filename_from_url(pdf_url)
                    
                    urls_data.append((viewer_url, pdf_url, filename))
            
            logger.info(f"Loaded {len(urls_data)} URLs from Excel file")
            
        except Exception as e:
            logger.error(f"Error reading Excel file: {e}")
        
        return urls_data
    
    def load_checkpoint(self):
        """Load checkpoint"""
        checkpoint_file = os.path.join(SAVE_DIR, 'checkpoint.txt')
        last_processed_index = 0
        
        if os.path.exists(checkpoint_file):
            try:
                with open(checkpoint_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.startswith('Last_Processed_Index:'):
                            last_processed_index = int(line.split(':')[1].strip())
                            break
                logger.info(f"Resuming from index {last_processed_index}")
            except:
                logger.info("Starting from beginning")
        
        return last_processed_index
    
    def save_checkpoint(self, index, total_urls):
        """Save checkpoint"""
        checkpoint_file = os.path.join(SAVE_DIR, 'checkpoint.txt')
        try:
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                f.write(f"Last_Processed_Index: {index}\n")
                f.write(f"Total_URLs: {total_urls}\n")
                f.write(f"Downloaded_URLs: {len(self.downloaded_urls)}\n")
        except Exception as e:
            logger.error(f"Error saving checkpoint: {e}")
    
    def filter_new_urls(self, urls_data, start_index=0):
        """Filter out already downloaded URLs"""
        urls_to_process = urls_data[start_index:]
        new_urls = [(viewer_url, pdf_url, filename) for viewer_url, pdf_url, filename in urls_to_process 
                   if viewer_url not in self.downloaded_urls]
        logger.info(f"Starting from index {start_index}, {len(new_urls)} URLs to download")
        return new_urls, start_index
    
    def generate_filename_from_url(self, url):
        """Generate filename from URL as fallback"""
        parsed = urlparse(url)
        path_parts = [p for p in parsed.path.split('/') if p]
        
        if path_parts:
            filename = "_".join(path_parts[-2:]) if len(path_parts) >= 2 else path_parts[-1]
        else:
            filename = "document"
        
        filename = re.sub(r'[^a-zA-Z0-9_-]', '_', filename)
        filename = filename.strip('_')
        
        if not filename.endswith('.pdf'):
            filename += '.pdf'
        
        return filename[:100]
    
    def download_pdf(self, pdf_url: str, filename: str) -> bool:
        """Download PDF with maximum speed using requests"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': 'https://www.selfstudys.com/',
                'Accept': 'application/pdf, */*',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            
            # Very short timeout for speed
            response = self.session.get(pdf_url, headers=headers, timeout=3, stream=False)
            
            if response.status_code == 200:
                filepath = os.path.join(SAVE_DIR, filename)
                
                # Write file directly
                with open(filepath, 'wb') as f:
                    f.write(response.content)
                
                # Quick validation
                if os.path.getsize(filepath) > 1024:
                    with open(filepath, 'rb') as f:
                        header = f.read(5)
                        if header.startswith(b'%PDF'):
                            return True
                        else:
                            os.remove(filepath)
                            return False
            return False
            
        except Exception as e:
            return False
    
    def save_metadata(self, viewer_url: str, pdf_url: str, filename: str):
        """Save download metadata to CSV"""
        csv_file = os.path.join(SAVE_DIR, 'downloads_metadata.csv')
        file_exists = os.path.isfile(csv_file)
        
        try:
            with open(csv_file, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                if not file_exists:
                    writer.writerow(['Timestamp', 'Viewer_URL', 'PDF_URL', 'Filename'])
                
                writer.writerow([
                    time.strftime('%Y-%m-%d %H:%M:%S'),
                    viewer_url,
                    pdf_url,
                    filename
                ])
        except Exception as e:
            logger.error(f"Error saving metadata: {e}")
    
    def process_single_url(self, viewer_url: str, pdf_url: str, filename: str) -> bool:
        """Process a single URL with maximum speed"""
        if viewer_url in self.downloaded_urls:
            return True
        
        # Check if file already exists to avoid re-downloading
        filepath = os.path.join(SAVE_DIR, filename)
        if os.path.exists(filepath) and os.path.getsize(filepath) > 1024:
            # Validate it's a PDF
            try:
                with open(filepath, 'rb') as f:
                    header = f.read(5)
                    if header.startswith(b'%PDF'):
                        self.downloaded_urls.add(viewer_url)
                        self.save_metadata(viewer_url, pdf_url, filename)
                        return True
            except:
                pass
        
        # Download the file
        success = self.download_pdf(pdf_url, filename)
        
        if success:
            self.downloaded_urls.add(viewer_url)
            self.save_metadata(viewer_url, pdf_url, filename)
        
        return success
    
    def process_url_batch(self, urls_batch, batch_num, total_batches):
        """Process a batch of URLs with maximum parallelism"""
        successful = 0
        failed = 0
        
        # Use ThreadPoolExecutor for maximum parallelism within the batch
        with ThreadPoolExecutor(max_workers=min(200, len(urls_batch))) as batch_executor:
            futures = {}
            
            for i, (viewer_url, pdf_url, filename) in enumerate(urls_batch):
                if self.interrupted:
                    break
                
                future = batch_executor.submit(self.process_single_url, viewer_url, pdf_url, filename)
                futures[future] = (viewer_url, pdf_url, filename)
            
            # Process results as they complete
            for future in as_completed(futures):
                if self.interrupted:
                    break
                
                viewer_url, pdf_url, filename = futures[future]
                try:
                    if future.result():
                        successful += 1
                    else:
                        failed += 1
                except:
                    failed += 1
                
                # Log progress every 100 URLs
                if (successful + failed) % 100 == 0:
                    logger.info(f"Batch {batch_num}/{total_batches}: Processed {successful + failed}/{len(urls_batch)} URLs")
        
        return successful, failed
    
    def run_ultra_fast_download(self, excel_file: str, batch_size: int = 5000):
        """Run the ultra-fast download process"""
        logger.info("ðŸš€ Starting Ultra-Fast PDF Downloader...")
        logger.info("=" * 60)
        logger.info(f"PDFs will be saved to: {SAVE_DIR}")
        logger.info(f"Max workers: {self.max_workers}")
        logger.info("=" * 60)
        
        # Reset checkpoint to start fresh
        start_index = 0
        self.save_checkpoint(0, 0)
        
        # Read URLs from Excel file
        all_urls_data = self.read_urls_from_excel(excel_file)
        
        if not all_urls_data:
            logger.info("No URLs to process. Exiting.")
            return
        
        # Filter out already downloaded URLs
        new_urls_data, current_index = self.filter_new_urls(all_urls_data, start_index)
        
        if not new_urls_data:
            logger.info("All URLs have already been downloaded. Exiting.")
            return
        
        logger.info(f"URLs to download: {len(new_urls_data)}")
        
        # Create very large batches for maximum throughput
        batches = [new_urls_data[i:i+batch_size] for i in range(0, len(new_urls_data), batch_size)]
        total_batches = len(batches)
        
        total_successful = 0
        total_failed = 0
        
        # Process batches in parallel with maximum concurrency
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            
            for batch_num, batch in enumerate(batches):
                if self.interrupted:
                    break
                    
                logger.info(f"Submitting batch {batch_num+1}/{total_batches} ({len(batch)} URLs)")
                futures.append(executor.submit(self.process_url_batch, batch, batch_num+1, total_batches))
            
            # Process results as they complete
            for future in as_completed(futures):
                if self.interrupted:
                    break
                    
                successful, failed = future.result()
                total_successful += successful
                total_failed += failed
                
                # Save progress
                current_index = start_index + total_successful + total_failed
                self.save_checkpoint(current_index, len(all_urls_data))
                
                # Log batch progress
                logger.info(f"Batch completed: {successful} successful, {failed} failed")
        
        # Print final summary
        logger.info("=" * 60)
        logger.info("ðŸ“Š ULTRA-FAST DOWNLOAD SUMMARY")
        logger.info(f"Total URLs processed: {len(new_urls_data)}")
        logger.info(f"Successful downloads: {total_successful}")
        logger.info(f"Failed downloads: {total_failed}")
        
        if len(new_urls_data) > 0:
            success_rate = (total_successful / len(new_urls_data)) * 100
            logger.info(f"Success rate: {success_rate:.1f}%")
        
        # Calculate and display performance metrics
        elapsed = time.time() - self.start_time
        hours = elapsed / 3600
        if hours > 0:
            urls_per_hour = total_successful / hours
            logger.info(f"Download speed: {urls_per_hour:.2f} URLs/hour")
        
        logger.info(f"Files saved in: {SAVE_DIR}")

# Run the downloader
if __name__ == "__main__":
    # Ultra-fast configuration
    EXCEL_FILE = "metadata.xlsx"  # Change this to your Excel file name
    MAX_WORKERS = 150  # Extremely high worker count for maximum parallelism
    BATCH_SIZE = 10000  # Massive batches for maximum throughput
    
    downloader = UltraFastPDFDownloader(max_workers=MAX_WORKERS)
    downloader.run_ultra_fast_download(excel_file=EXCEL_FILE, batch_size=BATCH_SIZE)
