import requests
from bs4 import BeautifulSoup
import re
import csv
import time
import os
import concurrent.futures

def get_pdf_download_url(page_url):
    """
    Extract PDF download link from SelfStudys book page - Optimized version
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # Fetch the page content with timeout
        response = requests.get(page_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Parse HTML content
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Look for PDF download links - optimized search
        pdf_links = []
        
        # Pattern 1: Direct PDF links in script tags (most common)
        script_tags = soup.find_all('script')
        for script in script_tags:
            if script.string:
                # Look for URLs containing 'sitepdfs'
                matches = re.findall(r'https://www\.selfstudys\.com/sitepdfs/[^\s"\']+', script.string)
                if matches:
                    pdf_links.extend(matches)
                    break  # Found in scripts, no need to check further
        
        # If not found in scripts, check anchor tags
        if not pdf_links:
            anchor_tags = soup.find_all('a', href=True)
            for anchor in anchor_tags:
                href = anchor['href']
                if 'sitepdfs' in href:
                    if href.startswith('//'):
                        href = 'https:' + href
                    elif href.startswith('/'):
                        href = 'https://www.selfstudys.com' + href
                    pdf_links.append(href)
                    break  # Found one, no need to check further
        
        # Return first found PDF URL
        return pdf_links[0] if pdf_links else None
        
    except requests.RequestException as e:
        return None
    except Exception as e:
        return None

def process_single_url(args):
    """Process a single URL and return result with index"""
    index, url = args
    pdf_url = get_pdf_download_url(url)
    
    if pdf_url:
        return index, url, pdf_url, "Success"
    else:
        return index, url, "", "Failed"

def process_urls_sequential(urls, delay=0.5):
    """Process URLs sequentially maintaining order"""
    results = []
    
    for i, url in enumerate(urls):
        print(f"[{i+1}/{len(urls)}] Processing: {url[:80]}...")
        
        start_time = time.time()
        pdf_url = get_pdf_download_url(url)
        end_time = time.time()
        
        if pdf_url:
            status = "Success"
            print(f"  ‚úì Success ({end_time - start_time:.2f}s): {pdf_url}")
        else:
            status = "Failed"
            print(f"  ‚úó Failed ({end_time - start_time:.2f}s)")
        
        results.append((i, url, pdf_url if pdf_url else "", status))
        
        # Smaller delay between requests
        if i < len(urls) - 1:
            time.sleep(delay)
    
    # Sort by original index to maintain order
    results.sort(key=lambda x: x[0])
    return [(url, pdf_url, status) for _, url, pdf_url, status in results]

def process_urls_parallel(urls, max_workers=3):
    """Process URLs in parallel while maintaining original order"""
    print(f"Processing {len(urls)} URLs in parallel with {max_workers} workers...")
    
    # Create list of (index, url) pairs
    url_tasks = [(i, url) for i, url in enumerate(urls)]
    
    results = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all tasks and store future to index mapping
        future_to_index = {}
        for index, url in url_tasks:
            future = executor.submit(process_single_url, (index, url))
            future_to_index[future] = index
        
        # Process completed tasks as they finish
        completed_count = 0
        for future in concurrent.futures.as_completed(future_to_index):
            index = future_to_index[future]
            completed_count += 1
            
            try:
                result = future.result(timeout=15)
                _, url, pdf_url, status = result
                
                if status == "Success":
                    print(f"[{completed_count}/{len(urls)}] ‚úì {url[:60]}...")
                    print(f"     PDF: {pdf_url}")
                else:
                    print(f"[{completed_count}/{len(urls)}] ‚úó {url[:60]}...")
                
                results.append(result)
            except concurrent.futures.TimeoutError:
                print(f"[{completed_count}/{len(urls)}] ‚úó Timeout: {urls[index][:60]}...")
                results.append((index, urls[index], "", "Failed - Timeout"))
            except Exception as e:
                print(f"[{completed_count}/{len(urls)}] ‚úó Error: {urls[index][:60]}... - {e}")
                results.append((index, urls[index], "", "Failed - Error"))
    
    # Sort results by original index to maintain input order
    results.sort(key=lambda x: x[0])
    return [(url, pdf_url, status) for _, url, pdf_url, status in results]

def read_urls_from_file(input_file):
    """Read URLs from file and return list maintaining order"""
    urls = []
    
    try:
        with open(input_file, 'r', encoding='utf-8') as file:
            for line_num, line in enumerate(file, 1):
                line = line.strip()
                if not line:
                    continue
                
                # Try to extract URL from the line
                if 'http' in line and 'selfstudys.com' in line:
                    # Extract URL using regex
                    url_match = re.search(r'(https?://[^\s<>"]+)', line)
                    if url_match:
                        urls.append(url_match.group(1))
                    else:
                        # If no URL pattern found but contains selfstudys.com, use the whole line
                        urls.append(line)
                elif line.startswith('http'):
                    # If it starts with http, assume it's a URL
                    urls.append(line)
        
        print(f"Found {len(urls)} URLs in input file")
        return urls
        
    except Exception as e:
        print(f"Error reading input file: {e}")
        return []

def save_results(results, output_txt, output_csv):
    """Save results to both text and CSV files"""
    # Ensure output directory exists
    os.makedirs(os.path.dirname(os.path.abspath(output_txt)) if os.path.dirname(output_txt) else '.', exist_ok=True)
    os.makedirs(os.path.dirname(os.path.abspath(output_csv)) if os.path.dirname(output_csv) else '.', exist_ok=True)
    
    # Save to text file
    try:
        with open(output_txt, 'w', encoding='utf-8') as txt_file:
            txt_file.write("Input URL | PDF Download URL | Status\n")
            txt_file.write("=" * 120 + "\n")
            for input_url, pdf_url, status in results:
                txt_file.write(f"{input_url} | {pdf_url} | {status}\n")
        print(f"‚úì Results saved to text file: {output_txt}")
    except Exception as e:
        print(f"‚úó Error writing to text file: {e}")
    
    # Save to CSV file
    try:
        with open(output_csv, 'w', newline='', encoding='utf-8') as csv_file:
            writer = csv.writer(csv_file)
            writer.writerow(['Input_URL', 'PDF_Download_URL', 'Status'])
            for input_url, pdf_url, status in results:
                writer.writerow([input_url, pdf_url, status])
        print(f"‚úì Results saved to CSV file: {output_csv}")
    except Exception as e:
        print(f"‚úó Error writing to CSV file: {e}")
    
    return os.path.exists(output_txt) and os.path.exists(output_csv)

def process_urls_from_file(input_file, output_txt, output_csv, use_parallel=True, delay=0.3):
    """
    Fast function to process URLs from file maintaining order
    """
    # Check if input file exists
    if not os.path.exists(input_file):
        print(f"Error: Input file '{input_file}' not found.")
        print(f"Current directory: {os.getcwd()}")
        return False
    
    # Read URLs from input file
    urls = read_urls_from_file(input_file)
    
    if not urls:
        print("No URLs found to process.")
        return False
    
    start_time = time.time()
    
    # Process URLs
    if use_parallel and len(urls) > 1:
        results = process_urls_parallel(urls, max_workers=3)
    else:
        results = process_urls_sequential(urls, delay=delay)
    
    total_time = time.time() - start_time
    
    # Save results
    files_created = save_results(results, output_txt, output_csv)
    
    # Print summary
    successful = sum(1 for _, _, status in results if status == "Success")
    failed = len(urls) - successful
    
    print(f"\n" + "="*60)
    print("PROCESSING SUMMARY")
    print("="*60)
    print(f"Total URLs processed: {len(urls)}")
    print(f"Successful extractions: {successful}")
    print(f"Failed extractions: {failed}")
    print(f"Success rate: {(successful/len(urls))*100:.1f}%")
    print(f"Total time: {total_time:.2f} seconds")
    print(f"Average time per URL: {total_time/len(urls):.2f} seconds")
    
    # Show output file locations
    if files_created:
        print(f"\nüìÅ Output files created:")
        print(f"   Text file: {os.path.abspath(output_txt)}")
        print(f"   CSV file: {os.path.abspath(output_csv)}")
    else:
        print(f"\n‚ùå Output files were not created successfully")
    
    return files_created

# Test function
def test_extraction():
    """Test the extraction with known working URL"""
    test_url = "https://www.selfstudys.com/books/bihar/state-books/class-12th/2024/english-105-set-e-2024/1103030"
    
    print("Testing PDF extraction...")
    start_time = time.time()
    pdf_url = get_pdf_download_url(test_url)
    end_time = time.time()
    
    if pdf_url:
        print(f"‚úì SUCCESS ({end_time - start_time:.2f}s): {pdf_url}")
    else:
        print(f"‚úó FAILED ({end_time - start_time:.2f}s)")
    return pdf_url is not None

# Main execution
if __name__ == "__main__":
    input_filename = "urls-4.txt"
    output_text = "pdf_urls_output.txt"
    output_csv = "pdf_urls_output.csv"
    
    print("üöÄ FAST SelfStudys PDF URL Extractor")
    print("="*50)
    print(f"Current directory: {os.getcwd()}")
    print(f"Input file: {input_filename}")
    print(f"Output files: {output_text}, {output_csv}")
    print("="*50)
    
    # First test if extraction works
    if test_extraction():
        print("\n" + "="*50)
        print("Starting bulk processing...")
        
        # Process all URLs
        success = process_urls_from_file(
            input_filename, 
            output_text, 
            output_csv, 
            use_parallel=True,  # Set to False for sequential processing
            delay=0.2  # Smaller delay between requests
        )
        
        if success:
            print(f"\nüéâ Processing completed successfully!")
            print(f"   Check the output files in: {os.getcwd()}")
        else:
            print(f"\n‚ùå Processing failed or no output files created")
    else:
        print("Basic test failed. Please check the website availability.")
