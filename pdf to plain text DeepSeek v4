import os
import requests
import json
from pathlib import Path
import time
import logging
from typing import Optional, List
import re
import pickle
import warnings

# Fix for PIL decompression bomb warnings
import PIL.Image
# Remove or increase the decompression bomb limit
PIL.Image.MAX_IMAGE_PIXELS = None  # Remove limit entirely
# Suppress the warnings
warnings.filterwarnings("ignore", category=PIL.Image.DecompressionBombWarning)

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ProgressTracker:
    def __init__(self, progress_file="pdf_processing_progress.pkl"):
        self.progress_file = Path(progress_file)
        self.processed_files = set()
        self.load_progress()
    
    def load_progress(self):
        """Load progress from file"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'rb') as f:
                    data = pickle.load(f)
                    self.processed_files = data.get('processed_files', set())
                logger.info(f"Loaded progress: {len(self.processed_files)} files already processed")
            except Exception as e:
                logger.warning(f"Could not load progress file: {e}")
                self.processed_files = set()
    
    def save_progress(self):
        """Save progress to file"""
        try:
            with open(self.progress_file, 'wb') as f:
                pickle.dump({'processed_files': self.processed_files}, f)
        except Exception as e:
            logger.error(f"Could not save progress: {e}")
    
    def mark_file_complete(self, file_path):
        """Mark a file as completely processed"""
        self.processed_files.add(str(file_path))
        self.save_progress()
    
    def is_file_processed(self, file_path):
        """Check if file is already processed"""
        return str(file_path) in self.processed_files

class DeepSeekRewriter:
    def __init__(self, api_key, temperature=0.1, max_tokens=8000):
        self.api_key = api_key
        self.base_url = "https://api.deepseek.com/v1/chat/completions"
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        self.temperature = temperature
        self.max_tokens = max_tokens
    
    def rewrite_content(self, content, retries=3):
        """Rewrite content using DeepSeek API with retry logic"""
        
        prompt = """
        You are an expert academic content rewriter. Please rewrite the following content to improve clarity, grammar, and readability while STRICTLY preserving:
        
        CRITICAL PRESERVATION RULES:
        1. ALL technical accuracy, formulas, equations, and mathematical notation exactly as written
        2. The original meaning and context without any interpretation
        3. Question numbers, options (A/B/C/D), and structure precisely
        4. All numerical values, units, and scientific notation
        5. Technical terminology and domain-specific language
        6. Multiple choice question formats exactly
        
        Focus only on improving:
        - Grammar and sentence structure
        - Spelling corrections
        - Punctuation
        - Readability of explanatory text
        
        Content to rewrite:
        """
        
        max_chunk_size = 6000
        if len(content) > max_chunk_size:
            chunks = self.split_content_preserving_questions(content, max_chunk_size)
            rewritten_chunks = []
            for i, chunk in enumerate(chunks):
                logger.info(f"Processing chunk {i+1}/{len(chunks)}...")
                rewritten_chunk = self.process_chunk_with_retry(chunk, prompt, retries)
                if rewritten_chunk and rewritten_chunk.strip():
                    rewritten_chunks.append(rewritten_chunk)
                time.sleep(2)
            return "\n\n".join(rewritten_chunks) if rewritten_chunks else content
        else:
            result = self.process_chunk_with_retry(content, prompt, retries)
            return result if result else content
    
    def process_chunk_with_retry(self, chunk, prompt, retries):
        """Process chunk with retry logic"""
        for attempt in range(retries):
            try:
                return self.process_chunk(chunk, prompt)
            except Exception as e:
                logger.warning(f"API call failed (attempt {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(8 * (attempt + 1))
                else:
                    logger.error("All retries failed. Returning original content.")
                    return chunk
    
    def process_chunk(self, chunk, prompt):
        """Process a single chunk of content"""
        if len(chunk.strip()) < 50:
            return chunk
            
        payload = {
            "model": "deepseek-chat",
            "messages": [
                {
                    "role": "system",
                    "content": "You are a technical content rewriter specializing in academic and engineering materials. You preserve all technical details exactly while improving readability."
                },
                {
                    "role": "user", 
                    "content": prompt + chunk
                }
            ],
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": 0.9
        }
        
        try:
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=100)
            response.raise_for_status()
            result = response.json()
            
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                if len(content.strip()) > len(chunk.strip()) * 0.1:
                    return content
                else:
                    logger.warning("API returned very short content, using original")
                    return chunk
            else:
                logger.warning("Unexpected API response format")
                return chunk
                
        except requests.exceptions.RequestException as e:
            logger.error(f"API request failed: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error: {e}")
            raise
    
    def split_content_preserving_questions(self, content, chunk_size):
        """Split content while preserving question boundaries"""
        chunks = []
        
        question_pattern = r'(Q\.\s*\d+[^\n]*(?:\n(?!Q\.\s*\d+).*)*)'
        questions = re.findall(question_pattern, content, re.IGNORECASE | re.DOTALL)
        
        if questions:
            current_chunk = ""
            for question in questions:
                if len(current_chunk) + len(question) < chunk_size:
                    current_chunk += question + "\n\n"
                else:
                    if current_chunk.strip():
                        chunks.append(current_chunk.strip())
                    current_chunk = question + "\n\n"
            if current_chunk.strip():
                chunks.append(current_chunk.strip())
        else:
            pages = content.split('===== Page')
            if len(pages) > 1:
                current_chunk = f"===== Page{pages[1]}"
                for page in pages[2:]:
                    page_content = f"===== Page{page}"
                    if len(current_chunk) + len(page_content) < chunk_size:
                        current_chunk += page_content
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = page_content
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
            else:
                paragraphs = content.split('\n\n')
                current_chunk = ""
                for para in paragraphs:
                    if len(current_chunk) + len(para) + 2 < chunk_size:
                        current_chunk += para + "\n\n"
                    else:
                        if current_chunk.strip():
                            chunks.append(current_chunk.strip())
                        current_chunk = para + "\n\n"
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
        
        return chunks

class AdvancedPDFProcessor:
    def __init__(self):
        self.extraction_methods = [
            self.try_simple_pymupdf,  # First try simple extraction for normal PDFs
            self.try_enhanced_pymupdf,
            self.try_enhanced_pdfplumber,
            self.try_pypdf2_extraction,
            self.try_enhanced_ocr  # OCR as last resort for scanned PDFs
        ]
    
    def extract_text_from_pdf(self, pdf_path, start_page=1, end_page=None):
        """Extract text from PDF - optimized for normal PDFs first"""
        logger.info(f"Extracting text from {pdf_path.name} (pages {start_page}-{end_page if end_page else 'end'})")
        
        all_extracted_text = ""
        
        for method in self.extraction_methods:
            try:
                logger.info(f"Trying {method.__name__}...")
                text = method(pdf_path, start_page, end_page)
                if text and self.is_substantial_text(text):
                    method_name = method.__name__.replace('try_', '').replace('_extraction', '')
                    logger.info(f"✓ Success with {method_name}")
                    
                    all_extracted_text = self.merge_extracted_text(all_extracted_text, text)
                    
                    # For normal PDFs, if we got good text with simple method, use it
                    if "simple" in method_name and len(text) > 500:
                        break
                    if len(text) > 1000:
                        break
                        
            except Exception as e:
                logger.debug(f"Method {method.__name__} failed: {e}")
                continue
        
        if all_extracted_text:
            cleaned_text = self.clean_extracted_text(all_extracted_text)
            return cleaned_text
        else:
            logger.error("✗ All extraction methods failed")
            return None

    def try_simple_pymupdf(self, pdf_path, start_page, end_page):
        """Simple PyMuPDF extraction - optimized for normal PDFs"""
        try:
            import fitz
            doc = fitz.open(pdf_path)
            
            total_pages = len(doc)
            if end_page is None or end_page > total_pages:
                end_page = total_pages
            
            text = ""
            for page_num in range(start_page - 1, end_page):
                page = doc[page_num]
                
                # Use simple text extraction first for normal PDFs
                page_text = page.get_text()
                
                # If simple extraction gives coordinate data (indication of scanned PDF), skip
                if self.looks_like_coordinate_data(page_text):
                    logger.info("Detected coordinate data, skipping simple extraction")
                    doc.close()
                    return ""
                
                text += f"===== Page {page_num + 1} =====\n\n{page_text}\n\n"
            
            doc.close()
            return text.strip()
        except Exception as e:
            raise Exception(f"Simple PyMuPDF error: {e}")

    def looks_like_coordinate_data(self, text):
        """Check if text looks like coordinate data instead of clean text"""
        if not text.strip():
            return False
        
        # Check for patterns that indicate coordinate data
        coordinate_patterns = [
            r'\(\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+,\s*\'[^\']*\',',  # (x1,y1,x2,y2,'text'
            r'\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+,\s*\d+\.\d+,\s*\'',  # x1,y1,x2,y2,'
            r'\(\s*\d+\s*,\s*\d+\s*,\s*\d+\s*,\s*\d+\s*,\s*\'',  # ( num, num, num, num, '
        ]
        
        lines = text.split('\n')
        coordinate_lines = 0
        total_lines = min(len(lines), 10)  # Check first 10 lines
        
        for line in lines[:10]:
            for pattern in coordinate_patterns:
                if re.search(pattern, line):
                    coordinate_lines += 1
                    break
        
        # If more than 30% of lines look like coordinate data, it's likely scanned
        return coordinate_lines > total_lines * 0.3
    
    def try_enhanced_pymupdf(self, pdf_path, start_page, end_page):
        """Enhanced PyMuPDF extraction for scanned PDFs"""
        try:
            import fitz
            doc = fitz.open(pdf_path)
            
            total_pages = len(doc)
            if end_page is None or end_page > total_pages:
                end_page = total_pages
            
            text = ""
            for page_num in range(start_page - 1, end_page):
                page = doc[page_num]
                
                # Try multiple extraction methods for scanned PDFs
                methods = [
                    lambda p: p.get_text(),  # Standard extraction
                    lambda p: p.get_text("words"),  # Word-based extraction
                    lambda p: p.get_text("blocks"),  # Block-based extraction
                ]
                
                page_text = ""
                for method in methods:
                    try:
                        result = method(page)
                        if isinstance(result, str):
                            extracted = result
                        else:
                            extracted = " ".join([str(item) for item in result])
                        
                        if len(extracted.strip()) > len(page_text):
                            page_text = extracted
                    except:
                        continue
                
                text += f"===== Page {page_num + 1} =====\n\n{page_text}\n\n"
            
            doc.close()
            return text.strip()
        except Exception as e:
            raise Exception(f"PyMuPDF error: {e}")
    
    def try_enhanced_pdfplumber(self, pdf_path, start_page, end_page):
        """Enhanced pdfplumber extraction with layout preservation"""
        try:
            import pdfplumber
            text = ""
            with pdfplumber.open(pdf_path) as pdf:
                total_pages = len(pdf.pages)
                if end_page is None or end_page > total_pages:
                    end_page = total_pages
                
                for page_num in range(start_page - 1, end_page):
                    page = pdf.pages[page_num]
                    
                    strategies = [
                        {"layout": True, "x_tolerance": 2, "y_tolerance": 2},
                        {"layout": False},
                        {"x_tolerance": 5, "y_tolerance": 5}
                    ]
                    
                    best_page_text = ""
                    for strategy in strategies:
                        try:
                            page_text = page.extract_text(**strategy) or ""
                            if len(page_text.strip()) > len(best_page_text.strip()):
                                best_page_text = page_text
                        except:
                            continue
                    
                    if len(best_page_text.strip()) < 100:
                        try:
                            chars = page.chars
                            if chars:
                                char_text = "".join([char['text'] for char in chars if 'text' in char])
                                if len(char_text) > len(best_page_text):
                                    best_page_text = char_text
                        except:
                            pass
                    
                    text += f"===== Page {page_num + 1} =====\n\n{best_page_text}\n\n"
            
            return text.strip()
        except Exception as e:
            raise Exception(f"pdfplumber error: {e}")
    
    def try_enhanced_ocr(self, pdf_path, start_page, end_page):
        """Enhanced OCR for scanned PDFs only"""
        try:
            from pdf2image import convert_from_path
            import pytesseract
            
            logger.info("Attempting OCR extraction for scanned PDF...")
            
            # Use lower DPI values to avoid decompression bomb warnings
            dpi_values = [200, 150, 250]  # Reduced from [400, 300, 500]
            
            best_text = ""
            for dpi in dpi_values:
                try:
                    if end_page:
                        images = convert_from_path(pdf_path, dpi=dpi, first_page=start_page, last_page=end_page)
                    else:
                        images = convert_from_path(pdf_path, dpi=dpi, first_page=start_page)
                    
                    current_text = ""
                    actual_start_page = start_page
                    
                    ocr_configs = [
                        r'--oem 3 --psm 6',
                        r'--oem 3 --psm 4',
                        r'--oem 3 --psm 3',
                        r'--oem 3 --psm 11'
                    ]
                    
                    for i, image in enumerate(images):
                        current_page = actual_start_page + i
                        logger.info(f"OCR processing page {current_page} at DPI {dpi}")
                        
                        page_text = ""
                        for config in ocr_configs:
                            try:
                                config_text = pytesseract.image_to_string(image, config=config, lang='eng')
                                if len(config_text.strip()) > len(page_text.strip()):
                                    page_text = config_text
                            except:
                                continue
                        
                        current_text += f"===== Page {current_page} =====\n\n{page_text}\n\n"
                    
                    if len(current_text.strip()) > len(best_text.strip()):
                        best_text = current_text
                        
                except Exception as e:
                    logger.warning(f"OCR failed at DPI {dpi}: {e}")
                    continue
            
            return best_text.strip()
            
        except ImportError as e:
            logger.warning(f"OCR libraries not available: {e}")
            return None
        except Exception as e:
            raise Exception(f"OCR error: {e}")
    
    def try_pypdf2_extraction(self, pdf_path, start_page, end_page):
        """Standard PyPDF2 extraction as fallback"""
        try:
            import PyPDF2
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                total_pages = len(reader.pages)
                if end_page is None or end_page > total_pages:
                    end_page = total_pages
                
                for page_num in range(start_page - 1, end_page):
                    page = reader.pages[page_num]
                    page_text = page.extract_text() or ""
                    text += f"===== Page {page_num + 1} =====\n\n{page_text}\n\n"
            
            return text.strip()
        except Exception as e:
            raise Exception(f"PyPDF2 error: {e}")
    
    def merge_extracted_text(self, existing_text, new_text):
        """Merge text from different extraction methods"""
        if not existing_text:
            return new_text
        
        if len(new_text.strip()) > len(existing_text.strip()):
            return new_text
        else:
            return existing_text
    
    def clean_extracted_text(self, text):
        """Clean and normalize extracted text"""
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                line = re.sub(r'\s+', ' ', line)
                cleaned_lines.append(line)
        
        cleaned_text = '\n'.join(cleaned_lines)
        
        replacements = {
            r'\\s+': ' ',
            r'Q\s*\.\s*': 'Q.',
            r'\(\s*A\s*\)': '(A)',
            r'\(\s*B\s*\)': '(B)', 
            r'\(\s*C\s*\)': '(C)',
            r'\(\s*D\s*\)': '(D)',
        }
        
        for pattern, replacement in replacements.items():
            cleaned_text = re.sub(pattern, replacement, cleaned_text)
        
        return cleaned_text
    
    def is_substantial_text(self, text, threshold=200):
        """Check if extracted text is substantial"""
        cleaned_text = re.sub(r'\s+', ' ', text).strip()
        
        has_questions = bool(re.search(r'Q\.\s*\d+', cleaned_text, re.IGNORECASE))
        has_options = bool(re.search(r'\([A-D]\)', cleaned_text))
        has_math = bool(re.search(r'[0-9]+\.[0-9]+|\d+', cleaned_text))
        
        return len(cleaned_text) >= threshold or has_questions or has_options or has_math
    
    def get_pdf_page_count(self, pdf_path):
        """Get total number of pages in PDF"""
        try:
            import fitz
            doc = fitz.open(pdf_path)
            page_count = len(doc)
            doc.close()
            return page_count
        except:
            try:
                import PyPDF2
                with open(pdf_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    return len(reader.pages)
            except:
                return None

def process_pdf_in_batches(pdf_file, api_key, progress_tracker, batch_size=5):
    """Process PDF in batches for better reliability"""
    pdf_processor = AdvancedPDFProcessor()
    rewriter = DeepSeekRewriter(api_key, temperature=0.1, max_tokens=8000)
    
    total_pages = pdf_processor.get_pdf_page_count(pdf_file)
    if total_pages is None:
        logger.error("Could not determine page count")
        return False
    
    logger.info(f"Total pages: {total_pages}, processing in batches of {batch_size}")
    
    all_rewritten_content = []
    
    for batch_start in range(1, total_pages + 1, batch_size):
        batch_end = min(batch_start + batch_size - 1, total_pages)
        
        logger.info(f"Processing batch: Pages {batch_start}-{batch_end}")
        
        content = pdf_processor.extract_text_from_pdf(pdf_file, batch_start, batch_end)
        
        if content and pdf_processor.is_substantial_text(content):
            logger.info(f"Extracted {len(content)} characters")
            
            logger.info("Rewriting content with AI...")
            try:
                rewritten_content = rewriter.rewrite_content(content)
                if rewritten_content and rewritten_content.strip():
                    all_rewritten_content.append(f"\n===== Pages {batch_start}-{batch_end} =====\n{rewritten_content}")
                    logger.info(f"✓ Batch {batch_start}-{batch_end} completed")
                else:
                    logger.warning("AI returned empty content, using original")
                    all_rewritten_content.append(f"\n===== Pages {batch_start}-{batch_end} =====\n{content}")
            except Exception as e:
                logger.error(f"Failed to rewrite batch {batch_start}-{batch_end}: {e}")
                all_rewritten_content.append(f"\n===== Pages {batch_start}-{batch_end} (Rewrite Failed) =====\n{content}")
        else:
            logger.warning(f"Poor extraction for pages {batch_start}-{batch_end}")
            if content:
                all_rewritten_content.append(f"\n===== Pages {batch_start}-{batch_end} (Poor Extraction) =====\n{content}")
            else:
                all_rewritten_content.append(f"\n===== Pages {batch_start}-{batch_end} (Extraction Failed) =====\n")
        
        time.sleep(4)
    
    if all_rewritten_content:
        final_content = "".join(all_rewritten_content)
        
        output_file = pdf_file.parent / f"rewritten_{pdf_file.stem}.txt"
        save_rewritten_content(final_content, output_file)
        
        progress_tracker.mark_file_complete(pdf_file)
        
        logger.info(f"✓ Successfully processed: {output_file.name}")
        return True
    else:
        logger.error("No content was processed successfully")
        return False

def find_all_pdf_files(root_directory):
    """Find all PDF files recursively in all subdirectories"""
    root_path = Path(root_directory)
    pdf_files = list(root_path.rglob("*.pdf"))
    return pdf_files

def process_all_pdf_files(root_directory, api_key):
    """Process all PDF files in the given directory and all subdirectories"""
    pdf_files = find_all_pdf_files(root_directory)
    
    if not pdf_files:
        logger.error(f"No PDF files found in {root_directory} and its subdirectories")
        return
    
    progress_tracker = ProgressTracker()
    
    files_to_process = [pdf for pdf in pdf_files if not progress_tracker.is_file_processed(pdf)]
    
    if not files_to_process:
        logger.info("All PDF files have already been processed!")
        return
    
    logger.info(f"Found {len(files_to_process)} PDF files to process ({len(pdf_files) - len(files_to_process)} already processed):")
    
    successful_count = len(pdf_files) - len(files_to_process)
    
    for i, pdf_file in enumerate(files_to_process, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"Processing file {i}/{len(files_to_process)}: {pdf_file.name}")
        logger.info(f"Full path: {pdf_file}")
        
        if process_pdf_in_batches(pdf_file, api_key, progress_tracker, batch_size=5):
            successful_count += 1
        
        logger.info(f"Progress: {i}/{len(files_to_process)} files completed")
        
        if i < len(files_to_process):
            logger.info("Waiting before processing next file...")
            time.sleep(12)
    
    logger.info(f"\n{'='*80}")
    logger.info(f"Processing complete!")
    logger.info(f"Successfully processed: {successful_count}/{len(pdf_files)} files")

def save_rewritten_content(content, output_path):
    """Save rewritten content to file"""
    try:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"Saved rewritten content to: {output_path}")
    except Exception as e:
        logger.error(f"Error saving file {output_path}: {e}")

def check_dependencies():
    """Check and install required dependencies"""
    required_packages = {
        'requests': 'requests',
        'PyPDF2': 'PyPDF2',
        'pdfplumber': 'pdfplumber',
        'pymupdf': 'fitz',
        'pdf2image': 'pdf2image',
        'pytesseract': 'pytesseract',
        'Pillow': 'PIL'
    }
    
    missing = []
    for package, import_name in required_packages.items():
        try:
            if import_name == 'fitz':
                import fitz
            elif import_name == 'PIL':
                from PIL import Image
            else:
                __import__(import_name)
        except ImportError:
            missing.append(package)
    
    if missing:
        logger.error("Missing dependencies. Please install:")
        logger.error(f"pip install {' '.join(missing)}")
        return False
    
    logger.info("All dependencies are available")
    return True

def main():
    API_KEY = "sk-467f5288c9ef40a4ae6ccec5978019ea"
    ROOT_DIRECTORY = r"D:\Vector Academy\Contents\CAT\CAT"
    
    if API_KEY == "your_deepseek_api_key_here" or len(API_KEY) < 20:
        logger.error("Please provide a valid DeepSeek API key")
        return
    
    if not os.path.exists(ROOT_DIRECTORY):
        logger.error(f"Directory {ROOT_DIRECTORY} does not exist!")
        return
    
    if not check_dependencies():
        logger.error("Please install the missing dependencies first.")
        return
    
    logger.info(f"Starting PDF processing from: {ROOT_DIRECTORY}")
    logger.info("This will process all PDF files in all subdirectories recursively.")
    logger.info("Output files will be saved in the same directories as the original PDFs.")
    logger.info("Progress will be automatically saved - if interrupted, run again to resume.")
    logger.info(f"{'='*80}")
    
    process_all_pdf_files(ROOT_DIRECTORY, API_KEY)

if __name__ == "__main__":
    main()
